{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "os.chdir('../')\n",
    "\n",
    "from src.data import read_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tagged_sentence(dir_path, tagged_tr, name_file, d_senses, mode='w', lemma=False, sel_verbs=True):\n",
    "    tagged_sense_sent = []\n",
    "    input_sense_sent = []\n",
    "    orig_sent = []\n",
    "    cont = 0\n",
    "    a = 0\n",
    "    for instance in tagged_tr:\n",
    "        temp_o = []\n",
    "        temp_i = []\n",
    "        for ix, word in enumerate(instance[1].split()):\n",
    "            if instance[0][ix] != 'no_instance':\n",
    "                if len(wordnet_senses) != 0:\n",
    "                    if sel_verbs and d_senses[instance[0][ix]][0] in wordnet_senses and wordnet_senses[d_senses[instance[0][ix]][0]] in SELECTED_SYNSETS:\n",
    "                        temp_o.append(wordnet_senses[d_senses[instance[0][ix]][0]])\n",
    "                        if lemma:\n",
    "                            d = nlp_pt(word)\n",
    "                            lemma = d[0].lemma_ \n",
    "                            if lemma == 'deixar-me':\n",
    "                                lemma = 'deixar'\n",
    "                            temp_i.append(lemma + '_tag')\n",
    "                        else:\n",
    "                            temp_i.append(word)\n",
    "                        a += 1\n",
    "                    elif not sel_verbs and d_senses[instance[0][ix]][0] in wordnet_senses:\n",
    "                        temp_o.append(wordnet_senses[d_senses[instance[0][ix]][0]])\n",
    "                        if lemma:\n",
    "                            d = nlp_pt(word)\n",
    "                            lemma = d[0].lemma_ \n",
    "                            if lemma == 'deixar-me':\n",
    "                                lemma = 'deixar'\n",
    "                            temp_i.append(lemma + '_tag')\n",
    "                        else:\n",
    "                            temp_i.append(word)\n",
    "                        a += 1\n",
    "                    else:\n",
    "                        cont += 1\n",
    "                        temp_o.append(word)\n",
    "                        temp_i.append(word)\n",
    "                else:\n",
    "                    temp_o.append(d_senses[instance[0][ix]][0])\n",
    "                    temp_i.append(word)\n",
    "            else:\n",
    "                temp_o.append(word)\n",
    "                temp_i.append(word)\n",
    "\n",
    "        input_sense_sent.append(' '.join(temp_i))\n",
    "        tagged_sense_sent.append(' '.join(temp_o))\n",
    "        orig_sent.append(instance[2])\n",
    "        \n",
    "    if lemma:\n",
    "        name_file += '_lemma'\n",
    "        \n",
    "    with open(dir_path / f'{name_file}_out.txt', mode) as file:\n",
    "        for sent in tagged_sense_sent:\n",
    "            file.write(sent + '\\n')\n",
    "    file.close()\n",
    "    \n",
    "    with open(dir_path / f'{name_file}_in.txt', mode) as file:\n",
    "        for sent in input_sense_sent:\n",
    "            file.write(sent + '\\n')\n",
    "    file.close() \n",
    "    \n",
    "    with open(dir_path / f'{name_file}_orig.txt', mode) as file:\n",
    "        for sent in orig_sent:\n",
    "            file.write(sent + '\\n')\n",
    "    file.close()     \n",
    "    \n",
    "    return cont, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PT_VERBS = ['tratar', 'estabelecer', 'marcar', 'vir', 'colocar',\\\n",
    "           'fechar', 'dar', 'cair', 'encontrar', 'registrar',\\\n",
    "           'levar', 'receber', 'apresentar', 'passar', 'deixar',\\\n",
    "           'chegar', 'ficar', 'fazer', 'ter', 'ser']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_SYNSETS = np.load(Path.cwd() / 'data/disambiguation/selected_synsets.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PT_DICT_VERBS ={\n",
    "    'tratar': ['deal with', 'treat', 'process', 'deal', 'manage', 'do', 'attend', 'look after', 'cherrish', 'misuse', 'size'],\\\n",
    "    'estabelecer': ['establish', 'set', 'fix', 'lay down', 'make', 'settle', 'determine', 'presribe', 'impose', 'enter into', 'stipulate', 'organize', 'seat'],\\\n",
    "    'marcar': ['mark', 'brand', 'dial', 'book', 'stamp', 'show', 'read', 'define', 'trace', 'earmark', 'reserve', 'feature', 'signilize', 'scribe'],\\\n",
    "    'vir': ['come', 'arrive', 'come on', 'come up with'],\\\n",
    "    'colocar': ['put', 'lay', 'set', 'place', 'post', 'pose', 'stick', 'plant', 'dispose', 'collocate', 'posit', 'bestow', 'pitch', 'clap', 'ship', 'placatory'],\\\n",
    "    'fechar': ['close', 'shut', 'seal', 'turn off', 'pin down', 'fasten', 'occlude', 'shut in', 'box', 'impount', 'bar', 'berate', 'stop up', 'mure', 'stopple', 'rail', 'pen', 'inmure'],\\\n",
    "    'dar': ['to give', 'give', 'impart', 'provide', 'render', 'afford', 'yield', 'allow', 'hand', 'deal', 'administer', 'give in', 'gift', 'confer', 'inflict', 'handsel', 'accord'],\\\n",
    "    'cair': ['fall', 'go down', 'drop', 'sink', 'collapse', 'founder', 'topple', 'lapse', 'sleet', 'keel over', 'prey', 'prostrate', 'pelt', 'plump', 'flump'],\\\n",
    "    'encontrar': ['meet', 'find', 'detect', 'encounter', 'find out', 'discover', 'meet with', 'experience', 'get together', 'impinge', 'hunt up'],\\\n",
    "    'registrar': ['register', 'record', 'read', 'book', 'enroll', 'inscribe', 'enrol', 'list', 'write down', 'set down', 'trace', 'score', 'label', 'matriculate', 'prick down', 'calendar', 'signalize'],\\\n",
    "    'levar': ['take along', 'take', 'carry', 'convey', 'go', 'prompt', 'induce', 'hold', 'charge', 'ravish'],\\\n",
    "    'receber': ['to receive', 'receive', 'welcome', 'get', 'have', 'accept', 'collect', 'meet', 'entertain', 'do', 'reap', 'derive', 'salute'],\\\n",
    "    'apresentar': ['to present', 'present', 'introduce', 'show', 'exhibit', 'lodge', 'produce', 'put', 'bring forward', 'come up with', 'represent', 'bring up', 'render'],\\\n",
    "    'passar': ['pass', 'spend', 'hand', 'go', 'go by', 'elapse', 'slip away', 'come', 'transfer', 'give in'],\\\n",
    "    'deixar': ['leave', 'let', 'have', 'quit', 'let go', 'depart', 'go away', 'drop off', 'leave out'],\\\n",
    "    'chegar': ['to arrive', 'arrive', ' get in', 'come', 'achieve', 'land', 'get around', 'turn up'],\\\n",
    "    'ficar': ['stay', 'bide', 'be', 'remain', 'go', 'continue', 'keep', 'come'],\\\n",
    "    'fazer': ['do', 'perform', 'make', 'cause', 'create', 'produce', 'render', 'manufacture'],\\\n",
    "    'ter': ['tue', 'have', 'take'],\\\n",
    "    'ser': ['to be', 'be', 'being']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval senses tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_file_semcor = open('data/translation/semcor/en_pt/semcor_or_2.txt', 'r')\n",
    "tr_file_semcor = open('data/translation/semcor/en_pt/semcor_tr_2.txt', 'r')\n",
    "id_file_semcor = open('data/translation/semcor/en_pt/semcor_id_2.txt', 'r')\n",
    "al_file_semcor = open('data/translation/semcor/en_pt/semcor_al_2.txt', 'r')\n",
    "\n",
    "or_file_omsti = open('data/translation/omsti/en_pt/omsti_or.txt', 'r')\n",
    "tr_file_omsti = open('data/translation/omsti/en_pt/omsti_tr.txt', 'r')\n",
    "id_file_omsti = open('data/translation/omsti/en_pt/omsti_id.txt', 'r')\n",
    "al_file_omsti = open('data/translation/omsti/en_pt/omsti_al.txt', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_alignment(chars):\n",
    "    alignment = []\n",
    "    for e in re.split(r'[,|;]', chars):\n",
    "        temp = e.split('-')\n",
    "        range_a = temp[0].split(':')\n",
    "        range_b = temp[1].split(':')\n",
    "        alignment.append([np.arange(int(range_a[0]), int(range_a[0]) + int(range_a[1]) + 1),\\\n",
    "                         np.arange(int(range_b[0]), int(range_b[0]) + int(range_b[1]) + 1)])\n",
    "        \n",
    "    return alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_arrays(or_file, tr_file, id_file, al_file):\n",
    "    tr_ar = []\n",
    "    for sent in tr_file:\n",
    "        tr_ar.append(sent.replace('\\n', ''))\n",
    "        \n",
    "    or_ar = []\n",
    "    for sent in or_file:\n",
    "        or_ar.append(sent.replace('\\n', ''))\n",
    "\n",
    "    id_ar = []\n",
    "    for sent in id_file:\n",
    "        id_ar.append(int(sent.replace('\\n', '')))\n",
    "\n",
    "    al_ar = []\n",
    "    for sent in al_file:\n",
    "        al_ar.append(parse_alignment(sent.replace('\\n', '')))\n",
    "        \n",
    "    return np.array(or_ar), np.array(tr_ar), np.array(id_ar), np.array(al_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_semcor, tr_semcor, id_semcor, al_semcor = load_arrays(or_file_semcor, tr_file_semcor, id_file_semcor, al_file_semcor)\n",
    "or_omsti, tr_omsti, id_omsti, al_omsti = load_arrays(or_file_omsti, tr_file_omsti, id_file_omsti, al_file_omsti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37163, 37163, 37163, 37163)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(or_semcor), len(tr_semcor), len(id_semcor), len(al_semcor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40260, 40260, 40260, 40260)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(or_omsti), len(tr_omsti), len(id_omsti), len(al_omsti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_file_semcor.close()\n",
    "tr_file_semcor.close()\n",
    "id_file_semcor.close()\n",
    "al_file_semcor.close()\n",
    "\n",
    "or_file_omsti.close()\n",
    "tr_file_omsti.close()\n",
    "id_file_omsti.close()\n",
    "al_file_omsti.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read corpus annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sent_semcor, sent_to_id_semcor, target_sent_semcor = read_sentences(Path.cwd() / 'data/WSD_Evaluation_Framework/Training_Corpora/', 'SemCor', False)\n",
    "input_sent_temp, sent_to_id_temp, target_sent_temp = read_sentences(Path.cwd() / 'data/WSD_Evaluation_Framework/Training_Corpora/', 'SemCor+OMSTI', True)\n",
    "\n",
    "# Filter sentences\n",
    "input_sent_omsti = []\n",
    "sent_to_id_omsti = []\n",
    "target_sent_omsti = []\n",
    "for ix, sent in enumerate(input_sent_temp):\n",
    "    if sent not in input_sent_semcor:\n",
    "        input_sent_omsti.append(sent)\n",
    "        sent_to_id_omsti.append(sent_to_id_temp[ix])\n",
    "        target_sent_omsti.append(target_sent_temp[ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check amount of valid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alignment(pos_word, al):\n",
    "    temp = []\n",
    "    for e in al:\n",
    "        if pos_word in e[1]:\n",
    "            temp.append(e)\n",
    "    \n",
    "    if len(temp):\n",
    "        temp = sorted(temp, key= lambda x: len(x[1]))\n",
    "        temp = temp[0]\n",
    "        \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words_with_verb(sent):\n",
    "    temp_ix = []\n",
    "    temp_cont = []\n",
    "    cont_char = 0\n",
    "    for ix, word in enumerate(sent.split()):\n",
    "        if word in PT_VERBS:\n",
    "            temp_cont.append(cont_char)\n",
    "            temp_ix.append(ix)\n",
    "        cont_char += len(word) + 1\n",
    "    return temp_cont, temp_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tagged_instance(id_arr, al_arr, tr_arr, input_sent_arr, sent_to_id_arr, target_sent_arr):\n",
    "    cont = 0\n",
    "    for ix_tr, ix_input_sent in enumerate(id_arr):\n",
    "        \n",
    "        input_sent = input_sent_arr[ix_input_sent]\n",
    "        sent_to_id = sent_to_id_arr[ix_input_sent]\n",
    "        target_sent = target_sent_arr[ix_input_sent]\n",
    "        \n",
    "        al = al_arr[ix_tr]\n",
    "        tr = tr_arr[ix_tr]\n",
    "        \n",
    "        arr_cont_char, arr_pos_word = count_words_with_verb(tr)\n",
    "        if len(arr_cont_char):\n",
    "            for cont_char, pos_word in zip(arr_cont_char, arr_pos_word):\n",
    "                alingment = get_alignment(cont_char, al)\n",
    "                #print(alingment)\n",
    "                t_or = min(alingment[0])\n",
    "                d_or = max(alingment[0])\n",
    "                t_tr = min(alingment[1])\n",
    "                d_tr = max(alingment[1])\n",
    "\n",
    "                ant_n_words = len(input_sent[:t_or].split())\n",
    "                n_words = len(input_sent[t_or:d_or].split())\n",
    "                for id_sense in sent_to_id[ant_n_words:(ant_n_words+n_words)]:\n",
    "                    if target_sent[id_sense] != 'no_instance':\n",
    "                        cont +=1\n",
    "    \n",
    "    return cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7280"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tagged_instance(id_semcor, al_semcor, tr_semcor, input_sent_semcor, sent_to_id_semcor, target_sent_semcor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6611"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tagged_instance(id_omsti, al_omsti, tr_omsti, input_sent_omsti, sent_to_id_omsti, target_sent_omsti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77423"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id_semcor) + len(id_omsti)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_file_semcor = open('data/translation/semcor/pt_en/semcor_or_2.txt', 'r')\n",
    "tr_file_semcor = open('data/translation/semcor/pt_en/semcor_tr_2.txt', 'r')\n",
    "al_file_semcor = open('data/translation/semcor/pt_en/semcor_al_2.txt', 'r')\n",
    "\n",
    "or_file_omsti = open('data/translation/omsti/pt_en/omsti_or_2.txt', 'r')\n",
    "tr_file_omsti = open('data/translation/omsti/pt_en/omsti_tr_2.txt', 'r')\n",
    "al_file_omsti = open('data/translation/omsti/pt_en/omsti_al_2.txt', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bck_or_semcor, bck_tr_semcor, _, bck_al_semcor = load_arrays(or_file_semcor, tr_file_semcor, [], al_file_semcor)\n",
    "bck_or_omsti, bck_tr_omsti, _, bck_al_omsti = load_arrays(or_file_omsti, tr_file_omsti, [], al_file_omsti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_file_semcor.close()\n",
    "tr_file_semcor.close()\n",
    "al_file_semcor.close()\n",
    "\n",
    "or_file_omsti.close()\n",
    "tr_file_omsti.close()\n",
    "al_file_omsti.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the next days may show where things are .',\n",
       " 'the next days may show where things stand .')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bck_tr_semcor[-100], or_semcor[-100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('whether as athletes or spectators , when we find ourselves in and participate in international sporting events , we share the brilliance of world-class competition , whether it be in the pain of losing, or the glory of winning , but more importantly , on a willingness of participation .',\n",
       " 'whether as athletes or spectators , when we meet at and participate in international sporting events , we share in the glow of world-class competition , whether it be in the heartbreak of losing or in the glory of winning , but most importantly , in the goodwill of participation .')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bck_tr_omsti[-100], or_omsti[-100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rouge_per_sentence(input_sentences, trns_sentences):\n",
    "    f1_scores = []\n",
    "    for sent, trns in zip(input_sentences, trns_sentences):\n",
    "        evaluator = rouge.Rouge(metrics=['rouge-n'],\n",
    "                               max_n=2,\n",
    "                               limit_length=True,\n",
    "                               length_limit=100,\n",
    "                               length_limit_type='words',\n",
    "                               alpha=0.5, # Default F1_score\n",
    "                               weight_factor=1.2,\n",
    "                               stemming=True)\n",
    "\n",
    "        all_hypothesis = [trns]\n",
    "        all_references = [sent]\n",
    "\n",
    "        scores = evaluator.get_scores(all_hypothesis, all_references)\n",
    "        for metric, results in sorted(scores.items(), key=lambda x: x[0]):\n",
    "            if metric == 'rouge-2':\n",
    "                f1_scores.append(results['f'])\n",
    "\n",
    "    return np.array(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_omsti = get_rouge_per_sentence(or_omsti, bck_tr_omsti)\n",
    "rouge_semcor = get_rouge_per_sentence(or_semcor, bck_tr_semcor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval Bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "def get_bleu_per_sentence(input_sentences, trns_sentences):\n",
    "    bleu_scores = []\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    for sent, trns in zip(input_sentences, trns_sentences):\n",
    "        hypothesis = trns.split() \n",
    "        reference = sent.split()\n",
    "        \n",
    "        references = [reference]\n",
    "        bleu = nltk.translate.bleu_score.sentence_bleu(references, hypothesis, smoothing_function=smoothie)\n",
    "        \n",
    "        bleu_scores.append(bleu)\n",
    "\n",
    "    return np.array(bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_omsti = get_bleu_per_sentence(or_omsti, bck_tr_omsti)\n",
    "bleu_semcor = get_bleu_per_sentence(or_semcor, bck_tr_semcor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bleu_per_sentence(bleu, rouge):\n",
    "    \n",
    "    f1 = []\n",
    "    for b, r in zip(bleu, rouge):\n",
    "        if b != 0 or r != 0:\n",
    "            f1.append(2 * (b * r) / (b + r))\n",
    "        else:\n",
    "            f1.append(0)\n",
    "        \n",
    "    return np.array(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_omsti = get_bleu_per_sentence(bleu_omsti, rouge_omsti)\n",
    "f1_semcor = get_bleu_per_sentence(bleu_semcor, rouge_semcor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval meteor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meteor_per_sentence(name):\n",
    "    \n",
    "    meteor = []\n",
    "    with open(f'data/meteor_output/{name}_meteor.txt') as f:\n",
    "        for line in f.read().split('\\n'):\n",
    "            if 'Segment' in line and 'score' in line:\n",
    "                meteor.append(float(line.split(':')[1]))\n",
    "        \n",
    "    return np.array(meteor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteor_omsti = get_meteor_per_sentence('omsti')\n",
    "meteor_semcor = get_meteor_per_sentence('semcor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer\n",
    "import re, string, unicodedata\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def normalize_text(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "import scipy \n",
    "\n",
    "def calc_vector(text, model):\n",
    "    item = html.unescape(text)\n",
    "    gen_doc = [w for w in word_tokenize(item)]\n",
    "    normalized = normalize_text(gen_doc)\n",
    "    \n",
    "    model.random.seed(SEED)\n",
    "    return model.infer_vector(normalized, epochs=6)\n",
    "\n",
    "def similarity(text1, text2, model):\n",
    "    vec1 = calc_vector(text1, model)\n",
    "    vec2 = calc_vector(text2, model)    \n",
    "    return 1-scipy.spatial.distance.cosine(vec1, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim_per_sentence(or_sents, bck_sents):\n",
    "    \n",
    "    sims = []\n",
    "    for or_s, bck_s in zip(or_sents, bck_sents):\n",
    "        sims.append(similarity(or_s, bck_s, model))\n",
    "        \n",
    "    return np.array(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "model = Doc2Vec.load('data/vectors/enwiki_doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# punkt\n",
    "# stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_omsti = get_sim_per_sentence(or_omsti, bck_tr_omsti)\n",
    "sim_semcor = get_sim_per_sentence(or_semcor, bck_tr_semcor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(sent):\n",
    "    temp_ix = []\n",
    "    temp_cont = []\n",
    "    cont_char = 0\n",
    "    for ix, word in enumerate(sent.split()):\n",
    "        temp_cont.append(cont_char)\n",
    "        temp_ix.append(ix)\n",
    "        \n",
    "        cont_char += len(word) + 1\n",
    "        \n",
    "    return temp_cont, temp_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_word(tr_word, orig_words):\n",
    "    if tr_word in PT_DICT_VERBS:\n",
    "        orig_options = PT_DICT_VERBS[tr_word]\n",
    "        for orig_word in orig_words:\n",
    "            if orig_word in orig_options:\n",
    "                return True\n",
    "                \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "class WhitespaceTokenizer(object):\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __call__(self, text):\n",
    "        words = text.split(' ')\n",
    "        # All tokens 'own' a subsequent space character in this tokenizer\n",
    "        spaces = [True] * len(words)\n",
    "        return Doc(self.vocab, words=words, spaces=spaces)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp_pt = spacy.load('pt_core_news_sm')\n",
    "nlp_pt.tokenizer = WhitespaceTokenizer(nlp_pt.vocab)\n",
    "\n",
    "nlp_en = spacy.load('en_core_web_sm')\n",
    "nlp_en.tokenizer = WhitespaceTokenizer(nlp_en.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tags(orig_sentences, trns_sentences): \n",
    "    tags_tr = []\n",
    "    for sent in (trns_sentences):\n",
    "        if len(sent):\n",
    "            tag_tr = []\n",
    "            temp = nlp_pt(str(sent).strip())\n",
    "            for token in temp:\n",
    "                tag_tr.append([token.text, token.pos_])\n",
    "            tags_tr.append(tag_tr)\n",
    "        else:\n",
    "            tags_tr.append(['invalid', 'invalid'])\n",
    "    \n",
    "    tags_or = []\n",
    "    for sent in (orig_sentences):\n",
    "        if len(sent):\n",
    "            tag_or = []\n",
    "            temp = nlp_en(str(sent).strip())\n",
    "            for token in temp:\n",
    "                tag_or.append([token.text, token.pos_])\n",
    "            tags_or.append(tag_or)\n",
    "        else:\n",
    "            tags_or.append(['invalid', 'invalid'])\n",
    "        \n",
    "    return tags_or, tags_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagged_translations(selected_id_tr, id_arr, al_arr, tr_arr, input_sent_arr, sent_to_id_arr, target_sent_arr, pos_or, pos_tr):\n",
    "    cont = 0\n",
    "    tagged_tr = []\n",
    "    \n",
    "    for ix_tr, ix_input_sent in zip(selected_id_tr, id_arr.take(selected_id_tr)):\n",
    "        input_sent = input_sent_arr[ix_input_sent]\n",
    "        sent_to_id = sent_to_id_arr[ix_input_sent]\n",
    "        target_sent = target_sent_arr[ix_input_sent]\n",
    "        \n",
    "        al = al_arr[ix_tr]\n",
    "        tr = tr_arr[ix_tr]\n",
    "        \n",
    "        arr_cont_char, arr_pos_word = count_words(tr)\n",
    "        if len(arr_cont_char):\n",
    "            \n",
    "            tags_tr = pos_tr[ix_tr]\n",
    "            tags_or = pos_or[ix_input_sent]\n",
    "            if verbose:\n",
    "                print()\n",
    "                print('sentence orig: ', input_sent)\n",
    "                print('sentence translated: ', tr)\n",
    "            temp = []\n",
    "            for cont_char, pos_word in zip(arr_cont_char, arr_pos_word):\n",
    "                alingment = get_alignment(cont_char, al)\n",
    "                if len(alingment):\n",
    "                    t_or = min(alingment[0])\n",
    "                    d_or = max(alingment[0])\n",
    "                    t_tr = min(alingment[1])\n",
    "                    d_tr = max(alingment[1])\n",
    "\n",
    "                    ant_n_words_or = len(input_sent[:t_or].split())\n",
    "                    n_words_or = len(input_sent[t_or:d_or].split())\n",
    "                    \n",
    "                    ant_n_words_tr = len(tr[:t_tr].split())\n",
    "                    n_words_tr = len(tr[t_tr:d_tr].split())\n",
    "                    \n",
    "                    input_sent_tokens = input_sent.split()[ant_n_words_or:(ant_n_words_or+n_words_or)]\n",
    "                    trns_sent_tokens = tr.split()[ant_n_words_tr:(ant_n_words_tr+n_words_tr)]\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print()\n",
    "                        print('translated word: ', tr.split()[pos_word])\n",
    "                        print('senses: ', np.array(target_sent).take(sent_to_id[ant_n_words_or:(ant_n_words_or+n_words_or)]))\n",
    "                        print('token input original: ', input_sent_tokens)\n",
    "                        print('token translations: ', tr.split()[ant_n_words_tr:(ant_n_words_tr+n_words_tr)])\n",
    "                        print('pos original: ', tags_tr[pos_word])\n",
    "                        print('pos translation: ', tags_or[ant_n_words_or:(ant_n_words_or+n_words_or)])\n",
    "                    if len(input_sent_tokens) == 1:   \n",
    "                        \n",
    "                        tag_or = tags_or[ant_n_words_or:(ant_n_words_or+n_words_or)][0]\n",
    "                        tag_tr = tags_tr[pos_word]\n",
    "                        id_sense = sent_to_id[ant_n_words_or:(ant_n_words_or+n_words_or)][0]\n",
    "                        if verbose:\n",
    "                            print('token 1: ', target_sent[id_sense])\n",
    "                        if target_sent[id_sense] != 'no_instance' and tag_or[1] == tag_tr[1]: \n",
    "                            temp.append(target_sent[id_sense])\n",
    "                        else:\n",
    "                            temp.append('no_instance') \n",
    "\n",
    "                    elif len(input_sent_tokens) <= 4 and len(trns_sent_tokens) <= 5 and check_word(tr.split()[pos_word], input_sent[t_or:d_or].split()):\n",
    "                        is_ambiguous = False\n",
    "\n",
    "                        for id_sense, word in zip(sent_to_id[ant_n_words_or:(ant_n_words_or+n_words_or)], input_sent_tokens):\n",
    "                            if target_sent[id_sense] != 'no_instance' and word in PT_DICT_VERBS[tr.split()[pos_word]]:\n",
    "                                is_ambiguous = True\n",
    "                                break\n",
    "                        \n",
    "                        if is_ambiguous:\n",
    "                            if verbose:\n",
    "                                print('token check_word: ', target_sent[id_sense])\n",
    "                            temp.append(target_sent[id_sense])\n",
    "                        else:\n",
    "                            temp.append('no_instance')\n",
    "                            \n",
    "                    elif len(input_sent_tokens) <= 4 and len(trns_sent_tokens) <= 4:   \n",
    "                        \n",
    "                        is_ambiguous = False\n",
    "                        tag_tr = tags_tr[pos_word]\n",
    "                        \n",
    "                        tag_cntr_or = Counter([tag[1] for tag in tags_or[ant_n_words_or:(ant_n_words_or+n_words_or)]])\n",
    "                        tag_cntr_tr = Counter([tag[1] for tag in tags_tr[ant_n_words_tr:(ant_n_words_tr+n_words_tr)]])\n",
    "\n",
    "                        for id_sense, tag_or in zip(sent_to_id[ant_n_words_or:(ant_n_words_or+n_words_or)], tags_or[ant_n_words_or:(ant_n_words_or+n_words_or)]):\n",
    "                            if target_sent[id_sense] != 'no_instance' and tag_or[1] == tag_tr[1] and tag_cntr_tr[tag_tr[1]] == 1 and tag_cntr_or[tag_or[1]] == 1: \n",
    "                                is_ambiguous = True\n",
    "                                temp_id = target_sent[id_sense]\n",
    "                                if verbose:\n",
    "                                    print('sentido encontrado: ', temp_id)\n",
    "                                break\n",
    "                                \n",
    "                        if is_ambiguous:\n",
    "                            temp.append(temp_id)\n",
    "                        else:\n",
    "                            temp.append('no_instance')\n",
    "\n",
    "                    else:\n",
    "                        temp.append('no_instance')\n",
    "                else:\n",
    "                    temp.append('no_instance')\n",
    "            \n",
    "            tagged_tr.append([temp, tr, input_sent])\n",
    "    \n",
    "    return tagged_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verbs(sent):\n",
    "    temp_cont = []\n",
    "    for ix, word in enumerate(sent.split()):\n",
    "        if word in PT_VERBS:\n",
    "            temp_cont.append(ix)\n",
    "    return temp_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_instances_with_verbs(tagged_tr):\n",
    "    cont = []\n",
    "    for ix, instance in enumerate(tagged_tr):\n",
    "        arr_pos_word = get_verbs(instance[1])\n",
    "        if len(arr_pos_word):\n",
    "            for pos_word in arr_pos_word:\n",
    "                if instance[0][pos_word] != 'no_instance':\n",
    "                    cont.append(ix)\n",
    "                    \n",
    "    return len(cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_index(metrics, sims, id_arr, p=''):\n",
    "    \n",
    "    filtered_ids = []\n",
    "    threshold_metric = np.mean(metrics) - np.std(metrics)\n",
    "    threshold_sim = np.mean(sims) - np.std(sims)\n",
    "\n",
    "    for ix, (metric, sim) in enumerate(zip(metrics, sims)):\n",
    "        if metric > threshold_metric:\n",
    "            filtered_ids.append(ix)\n",
    "        elif sim > threshold_sim:\n",
    "            filtered_ids.append(ix)\n",
    "                \n",
    "    print(f'{p:15s}total ids: {len(id_arr)}\\t|\\t After filter: {len(filtered_ids)}')\n",
    "    return filtered_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering semcor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter id's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge          total ids: 37163\t|\t After filter: 35294\n",
      "bleu           total ids: 37163\t|\t After filter: 35056\n",
      "f1             total ids: 37163\t|\t After filter: 35189\n",
      "m              total ids: 37163\t|\t After filter: 35154\n"
     ]
    }
   ],
   "source": [
    "filtered_ids_semcor_r = filter_index(rouge_semcor, sim_semcor, id_semcor, p='rouge')\n",
    "filtered_ids_semcor_b = filter_index(bleu_semcor, sim_semcor, id_semcor, p='bleu')\n",
    "filtered_ids_semcor_r_b = filter_index(f1_semcor, sim_semcor, id_semcor, p='f1')\n",
    "filtered_ids_semcor_m = filter_index(meteor_semcor, sim_semcor, id_semcor, p='m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pos_or, pos_tr = get_pos_tags(input_sent_semcor, tr_semcor)\n",
    "#np.save('pos_or.npy', pos_or)\n",
    "#np.save('pos_tr.npy', pos_tr)\n",
    "\n",
    "pos_or = np.load('data/serialize/pos_or_semcor.npy')\n",
    "pos_tr = np.load('data/serialize/pos_tr_semcor.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagging with rouge\n",
      "tagging with bleu\n",
      "tagging with f1\n",
      "tagging with meteor\n"
     ]
    }
   ],
   "source": [
    "verbose=False\n",
    "tagged_tr_semcor = tagged_translations(np.arange(len(id_semcor)), id_semcor, al_semcor, tr_semcor, input_sent_semcor, sent_to_id_semcor, target_sent_semcor, pos_or, pos_tr)\n",
    "print('tagging with rouge')\n",
    "tagged_tr_semcor_r = tagged_translations(filtered_ids_semcor_r, id_semcor, al_semcor, tr_semcor, input_sent_semcor, sent_to_id_semcor, target_sent_semcor, pos_or, pos_tr)\n",
    "print('tagging with bleu')\n",
    "tagged_tr_semcor_b = tagged_translations(filtered_ids_semcor_b, id_semcor, al_semcor, tr_semcor, input_sent_semcor, sent_to_id_semcor, target_sent_semcor, pos_or, pos_tr)\n",
    "print('tagging with f1')\n",
    "tagged_tr_semcor_r_b = tagged_translations(filtered_ids_semcor_r_b, id_semcor, al_semcor, tr_semcor, input_sent_semcor, sent_to_id_semcor, target_sent_semcor, pos_or, pos_tr)\n",
    "print('tagging with meteor')\n",
    "tagged_tr_semcor_m = tagged_translations(filtered_ids_semcor_m, id_semcor, al_semcor, tr_semcor, input_sent_semcor, sent_to_id_semcor, target_sent_semcor, pos_or, pos_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37163, 37163)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tagged_tr_semcor), len(id_semcor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3183"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for instance in tagged_tr_semcor_r:\n",
    "    if len(instance[0]) != len(instance[1].split()):\n",
    "        print(False)\n",
    "count_instances_with_verbs(tagged_tr_semcor_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3167"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for instance in tagged_tr_semcor_b:\n",
    "    if len(instance[0]) != len(instance[1].split()):\n",
    "        print(False)\n",
    "count_instances_with_verbs(tagged_tr_semcor_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3174"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for instance in tagged_tr_semcor_r_b:\n",
    "    if len(instance[0]) != len(instance[1].split()):\n",
    "        print(False)\n",
    "count_instances_with_verbs(tagged_tr_semcor_r_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3180"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for instance in tagged_tr_semcor_m:\n",
    "    if len(instance[0]) != len(instance[1].split()):\n",
    "        print(False)\n",
    "count_instances_with_verbs(tagged_tr_semcor_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering omsti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter id's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge          total ids: 40260\t|\t After filter: 37483\n",
      "bleu           total ids: 40260\t|\t After filter: 37500\n",
      "bleu-rouge     total ids: 40260\t|\t After filter: 37458\n",
      "bleu-rouge     total ids: 40260\t|\t After filter: 39042\n"
     ]
    }
   ],
   "source": [
    "filtered_ids_omsti_r = filter_index(rouge_omsti, sim_omsti, id_omsti, p='rouge')\n",
    "filtered_ids_omsti_b = filter_index(bleu_omsti, sim_omsti, id_omsti, p='bleu')\n",
    "filtered_ids_omsti_r_b = filter_index(f1_omsti, sim_omsti, id_omsti, p='bleu-rouge')\n",
    "filtered_ids_omsti_m = filter_index(meteor_omsti, sim_omsti, id_omsti, p='bleu-rouge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pos_or_omsti, pos_tr_omsti = get_pos_tags(input_sent_omsti, tr_omsti)\n",
    "#np.save('pos_or_omsti.npy', pos_or_omsti)\n",
    "#np.save('pos_tr_omsti.npy', pos_tr_omsti)\n",
    "\n",
    "pos_or_omsti = np.load('data/serialize/pos_or_omsti.npy')\n",
    "pos_tr_omsti = np.load('data/serialize/pos_tr_omsti.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagging with rouge\n",
      "tagging with bleu\n",
      "tagging with f1\n",
      "tagging with meteor\n"
     ]
    }
   ],
   "source": [
    "verbose=False\n",
    "tagged_tr_omsti = tagged_translations(np.arange(len(id_omsti)), id_omsti, al_omsti, tr_omsti, input_sent_omsti, sent_to_id_omsti, target_sent_omsti, pos_or_omsti, pos_tr_omsti)\n",
    "print('tagging with rouge')\n",
    "tagged_tr_omsti_r = tagged_translations(filtered_ids_omsti_r, id_omsti, al_omsti, tr_omsti, input_sent_omsti, sent_to_id_omsti, target_sent_omsti, pos_or_omsti, pos_tr_omsti)\n",
    "print('tagging with bleu')\n",
    "tagged_tr_omsti_b = tagged_translations(filtered_ids_omsti_b, id_omsti, al_omsti, tr_omsti, input_sent_omsti, sent_to_id_omsti, target_sent_omsti, pos_or_omsti, pos_tr_omsti)\n",
    "print('tagging with f1')\n",
    "tagged_tr_omsti_r_b = tagged_translations(filtered_ids_omsti_r_b, id_omsti, al_omsti, tr_omsti, input_sent_omsti, sent_to_id_omsti, target_sent_omsti, pos_or_omsti, pos_tr_omsti)\n",
    "print('tagging with meteor')\n",
    "tagged_tr_omsti_m = tagged_translations(filtered_ids_omsti_m, id_omsti, al_omsti, tr_omsti, input_sent_omsti, sent_to_id_omsti, target_sent_omsti, pos_or_omsti, pos_tr_omsti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4702"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for instance in tagged_tr_omsti_r:\n",
    "    if len(instance[0]) != len(instance[1].split()):\n",
    "        print(False)\n",
    "count_instances_with_verbs(tagged_tr_omsti_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4695"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for instance in tagged_tr_omsti_b:\n",
    "    if len(instance[0]) != len(instance[1].split()):\n",
    "        print(False)\n",
    "count_instances_with_verbs(tagged_tr_omsti_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4701"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for instance in tagged_tr_omsti_r_b:\n",
    "    if len(instance[0]) != len(instance[1].split()):\n",
    "        print(False)\n",
    "count_instances_with_verbs(tagged_tr_omsti_r_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4872"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for instance in tagged_tr_omsti_m:\n",
    "    if len(instance[0]) != len(instance[1].split()):\n",
    "        print(False)\n",
    "count_instances_with_verbs(tagged_tr_omsti_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_senses = {}\n",
    "with open(Path.cwd() / 'data/wordnet/index.sense') as f:\n",
    "    for line in f.read().split('\\n'):\n",
    "        if len(line) > 1:\n",
    "            wordnet_senses[line.split()[0]] = line.split()[1]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_senses_semcor = {}\n",
    "with open(Path.cwd() / 'data/WSD_Evaluation_Framework/Training_Corpora/SemCor/semcor.gold.key.txt', 'r') as f:\n",
    "    for line in f.read().split('\\n'):\n",
    "        if len(line):\n",
    "            temp = line.split()\n",
    "            d_senses_semcor[temp[0]] = temp[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_senses_omsti = {}\n",
    "with open(Path.cwd() / 'data/WSD_Evaluation_Framework/Training_Corpora/SemCor+OMSTI/semcor+omsti.gold.key.txt', 'r') as f:\n",
    "    for line in f.read().split('\\n'):\n",
    "        if len(line):\n",
    "            temp = line.split()\n",
    "            d_senses_omsti[temp[0]] = temp[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate Semcor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "semcor_path = Path.cwd() / 'data/disambiguation/semcor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================= Without Lemma =================\n",
      "(119425, 20573)\n",
      "================= With Lemma =================\n",
      "(119425, 20573)\n"
     ]
    }
   ],
   "source": [
    "print('================= Without Lemma =================')\n",
    "print(save_tagged_sentence(semcor_path, tagged_tr_semcor, 'semcor_map', d_senses_semcor, lemma=False, sel_verbs=True))\n",
    "\n",
    "print('================= With Lemma =================')\n",
    "print(save_tagged_sentence(semcor_path, tagged_tr_semcor, 'semcor_map', d_senses_semcor, lemma=True, sel_verbs=True))\n",
    "\n",
    "np.save(Path.cwd() / 'data/disambiguation/semcor' / 'rouge.npy', rouge_semcor)\n",
    "np.save(Path.cwd() / 'data/disambiguation/semcor' / 'bleu.npy', bleu_semcor)\n",
    "np.save(Path.cwd() / 'data/disambiguation/semcor' / 'f1.npy', f1_semcor)\n",
    "np.save(Path.cwd() / 'data/disambiguation/semcor' / 'meteor.npy', meteor_semcor)\n",
    "np.save(Path.cwd() / 'data/disambiguation/semcor' / 'sim.npy', sim_semcor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving rouge\n",
      "(116124, 19891)\n",
      "Saving bleu\n",
      "(115550, 19771)\n",
      "Saving rouge and bleu\n",
      "(115934, 19850)\n",
      "Saving meteor\n",
      "(115589, 19790)\n",
      "Saving rouge\n",
      "(116124, 19891)\n",
      "Saving bleu\n",
      "(115550, 19771)\n",
      "Saving rouge and bleu\n",
      "(115934, 19850)\n",
      "Saving meteor\n",
      "(115589, 19790)\n"
     ]
    }
   ],
   "source": [
    "for lemma_opt in [False, True]:\n",
    "    ################################################################################################\n",
    "    print('Saving rouge')\n",
    "    print(save_tagged_sentence(semcor_path, tagged_tr_semcor_r, 'semcor_rouge', d_senses_semcor, lemma=lemma_opt, sel_verbs=True))\n",
    "    np.save(semcor_path / 'semcor_rouge.npy', rouge_semcor.take(filtered_ids_semcor_r))\n",
    "    \n",
    "    np.random.seed(SEED + 10)\n",
    "    ix_random = np.random.choice(len(tagged_tr_semcor_r), len(filtered_ids_semcor_r), replace=False)\n",
    "    save_tagged_sentence(semcor_path, np.array(tagged_tr_semcor)[ix_random], 'semcor_rouge_random', d_senses_semcor, lemma=lemma_opt, sel_verbs=True)\n",
    "    np.save(semcor_path / 'semcor_rouge_random.npy', rouge_semcor.take(ix_random))\n",
    "\n",
    "    ################################################################################################\n",
    "    print('Saving bleu')\n",
    "    print(save_tagged_sentence(semcor_path, tagged_tr_semcor_b, 'semcor_bleu', d_senses_semcor, lemma=lemma_opt, sel_verbs=True))\n",
    "    np.save(semcor_path / 'semcor_bleu.npy', bleu_semcor.take(filtered_ids_semcor_b))\n",
    "    \n",
    "    np.random.seed(SEED + 20)\n",
    "    ix_random = np.random.choice(len(tagged_tr_semcor_b), len(filtered_ids_semcor_b), replace=False)\n",
    "    save_tagged_sentence(semcor_path, np.array(tagged_tr_semcor)[ix_random], 'semcor_bleu_random', d_senses_semcor, lemma=lemma_opt, sel_verbs=True)\n",
    "    np.save(semcor_path / 'semcor_bleu_random.npy', bleu_semcor.take(ix_random))\n",
    "    \n",
    "    ################################################################################################\n",
    "    print('Saving rouge and bleu')\n",
    "    print(save_tagged_sentence(semcor_path, tagged_tr_semcor_r_b, 'semcor_f1', d_senses_semcor, lemma=lemma_opt, sel_verbs=True))\n",
    "    np.save(semcor_path / 'semcor_f1.npy', f1_semcor.take(filtered_ids_semcor_r_b))\n",
    "    \n",
    "    np.random.seed(SEED + 30)\n",
    "    ix_random = np.random.choice(len(tagged_tr_semcor_r_b), len(filtered_ids_semcor_r_b), replace=False)\n",
    "    save_tagged_sentence(semcor_path, np.array(tagged_tr_semcor)[ix_random], 'semcor_f1_random', d_senses_semcor, lemma=lemma_opt, sel_verbs=True)\n",
    "    np.save(semcor_path / 'semcor_f1_random.npy', f1_semcor.take(ix_random))\n",
    "\n",
    "    ################################################################################################\n",
    "    print('Saving meteor')\n",
    "    print(save_tagged_sentence(semcor_path, tagged_tr_semcor_m, 'semcor_meteor', d_senses_semcor, lemma=lemma_opt, sel_verbs=True))\n",
    "    np.save(semcor_path / 'semcor_meteor.npy', meteor_semcor.take(filtered_ids_semcor_m))\n",
    "    \n",
    "    np.random.seed(SEED + 30)\n",
    "    ix_random = np.random.choice(len(tagged_tr_semcor_m), len(filtered_ids_semcor_m), replace=False)\n",
    "    save_tagged_sentence(semcor_path, np.array(tagged_tr_semcor)[ix_random], 'semcor_meteor_random', d_senses_semcor, lemma=lemma_opt, sel_verbs=True)\n",
    "    np.save(semcor_path / 'semcor_meteor_random.npy', meteor_semcor.take(ix_random))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate Omsti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "omsti_path = Path.cwd() / 'data/disambiguation/omsti'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================= Without Lemma =================\n",
      "(18025, 6811)\n",
      "================= With Lemma =================\n",
      "(18025, 6811)\n"
     ]
    }
   ],
   "source": [
    "print('================= Without Lemma =================')\n",
    "print(save_tagged_sentence(omsti_path, tagged_tr_omsti, 'omsti_map', d_senses_omsti, lemma=False, sel_verbs=True))\n",
    "\n",
    "print('================= With Lemma =================')\n",
    "print(save_tagged_sentence(omsti_path, tagged_tr_omsti, 'omsti_map', d_senses_omsti, lemma=True, sel_verbs=True))\n",
    "\n",
    "np.save(Path.cwd() / 'data/disambiguation/omsti' / 'rouge.npy', rouge_omsti)\n",
    "np.save(Path.cwd() / 'data/disambiguation/omsti' / 'bleu.npy', bleu_omsti)\n",
    "np.save(Path.cwd() / 'data/disambiguation/omsti' / 'f1.npy', f1_omsti)\n",
    "np.save(Path.cwd() / 'data/disambiguation/omsti' / 'meteor.npy', meteor_omsti)\n",
    "np.save(Path.cwd() / 'data/disambiguation/omsti' / 'sim.npy', sim_omsti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving rouge\n",
      "(16761, 6410)\n",
      "Saving bleu\n",
      "(16768, 6434)\n",
      "Saving rouge and bleu\n",
      "(16752, 6419)\n",
      "Saving meteor\n",
      "(17491, 6625)\n",
      "Saving rouge\n",
      "(16761, 6410)\n",
      "Saving bleu\n",
      "(16768, 6434)\n",
      "Saving rouge and bleu\n",
      "(16752, 6419)\n",
      "Saving meteor\n",
      "(17491, 6625)\n"
     ]
    }
   ],
   "source": [
    "for lemma_opt in [False, True]:\n",
    "    print('Saving rouge')\n",
    "    print(save_tagged_sentence(omsti_path, tagged_tr_omsti_r, 'omsti_rouge', d_senses_omsti, lemma=lemma_opt, sel_verbs=True))\n",
    "    np.save(omsti_path / 'omsti_rouge.npy', rouge_omsti.take(filtered_ids_omsti_r))\n",
    "    np.random.seed(SEED + 10)\n",
    "    ix_random = np.random.choice(len(tagged_tr_omsti_r), len(filtered_ids_omsti_r), replace=False)\n",
    "    save_tagged_sentence(omsti_path, np.array(tagged_tr_omsti)[ix_random], 'omsti_rouge_random', d_senses_omsti, lemma=lemma_opt, sel_verbs=True)\n",
    "    np.save(omsti_path / 'omsti_rouge_random.npy', rouge_omsti.take(ix_random))\n",
    "    \n",
    "    print('Saving bleu')\n",
    "    print(save_tagged_sentence(omsti_path, tagged_tr_omsti_b, 'omsti_bleu', d_senses_omsti, lemma=lemma_opt, sel_verbs=True))\n",
    "    np.save(omsti_path / 'omsti_bleu.npy', bleu_omsti.take(filtered_ids_omsti_b))\n",
    "    np.random.seed(SEED + 20)\n",
    "    ix_random = np.random.choice(len(tagged_tr_omsti_b), len(filtered_ids_omsti_b), replace=False)\n",
    "    save_tagged_sentence(omsti_path, np.array(tagged_tr_omsti)[ix_random], 'omsti_bleu_random', d_senses_omsti, lemma=lemma_opt, sel_verbs=True)\n",
    "    np.save(omsti_path / 'omsti_bleu_random.npy', bleu_omsti.take(ix_random))\n",
    "    \n",
    "    print('Saving rouge and bleu')\n",
    "    print(save_tagged_sentence(omsti_path, tagged_tr_omsti_r_b, 'omsti_f1', d_senses_omsti, lemma=lemma_opt, sel_verbs=True))\n",
    "    np.save(omsti_path / 'omsti_f1.npy', f1_omsti.take(filtered_ids_omsti_r_b))\n",
    "    np.random.seed(SEED + 30)\n",
    "    ix_random = np.random.choice(len(tagged_tr_omsti_r_b), len(filtered_ids_omsti_r_b), replace=False)\n",
    "    save_tagged_sentence(omsti_path, np.array(tagged_tr_omsti)[ix_random], 'omsti_f1_random', d_senses_omsti, lemma=lemma_opt, sel_verbs=True)\n",
    "    np.save(omsti_path / 'omsti_f1_random.npy', f1_omsti.take(ix_random))\n",
    "   \n",
    "    print('Saving meteor')\n",
    "    print(save_tagged_sentence(omsti_path, tagged_tr_omsti_m, 'omsti_meteor', d_senses_omsti, lemma=lemma_opt, sel_verbs=True))\n",
    "    np.save(omsti_path / 'omsti_m.npy', meteor_omsti.take(filtered_ids_omsti_m))\n",
    "    np.random.seed(SEED + 30)\n",
    "    ix_random = np.random.choice(len(tagged_tr_omsti_m), len(filtered_ids_omsti_m), replace=False)\n",
    "    save_tagged_sentence(omsti_path, np.array(tagged_tr_omsti)[ix_random], 'omsti_meteor_random', d_senses_omsti, lemma=lemma_opt, sel_verbs=True)\n",
    "    np.save(omsti_path / 'omsti_meteor_random.npy', meteor_omsti.take(ix_random))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_path = Path.cwd() / 'data/disambiguation/all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================= Without Lemma =================\n",
      "(18025, 6811)\n",
      "================= With Lemma =================\n",
      "(18025, 6811)\n"
     ]
    }
   ],
   "source": [
    "print('================= Without Lemma =================')\n",
    "print(save_tagged_sentence(all_path, tagged_tr_omsti, 'all', d_senses_omsti, lemma=False, sel_verbs=True))\n",
    "save_tagged_sentence(all_path, tagged_tr_semcor, 'all', d_senses_semcor, lemma=False, sel_verbs=True, mode='w')\n",
    "\n",
    "print('================= With Lemma =================')\n",
    "print(save_tagged_sentence(all_path, tagged_tr_omsti, 'all', d_senses_omsti, lemma=True, sel_verbs=True))\n",
    "save_tagged_sentence(all_path, tagged_tr_semcor, 'all', d_senses_semcor, lemma=True, sel_verbs=True, mode='w')\n",
    "\n",
    "np.save(all_path / 'rouge.npy', np.concatenate((rouge_omsti, rouge_semcor)))\n",
    "np.save(all_path / 'bleu.npy', np.concatenate((bleu_omsti, bleu_semcor)))\n",
    "np.save(all_path / 'f1.npy', np.concatenate((f1_omsti, f1_semcor)))\n",
    "np.save(all_path / 'meteor.npy', np.concatenate((meteor_omsti, meteor_semcor)))\n",
    "np.save(all_path / 'sim.npy', np.concatenate((sim_omsti, sim_semcor)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving rouge\n",
      "(16761, 6410)\n",
      "(116124, 19891)\n",
      "Saving bleu\n",
      "(16768, 6434)\n",
      "(115550, 19771)\n",
      "Saving rouge and bleu\n",
      "(16752, 6419)\n",
      "(115934, 19850)\n",
      "Saving meteor\n",
      "(17491, 6625)\n",
      "(115589, 19790)\n",
      "Saving rouge\n",
      "(16761, 6410)\n",
      "(116124, 19891)\n",
      "Saving bleu\n",
      "(16768, 6434)\n",
      "(115550, 19771)\n",
      "Saving rouge and bleu\n",
      "(16752, 6419)\n",
      "(115934, 19850)\n",
      "Saving meteor\n",
      "(17491, 6625)\n",
      "(115589, 19790)\n"
     ]
    }
   ],
   "source": [
    "for lemma_opt in [False, True]:\n",
    "    print('Saving rouge')\n",
    "    print(save_tagged_sentence(all_path, tagged_tr_omsti_r, 'all_rouge', d_senses_omsti, lemma=lemma_opt, sel_verbs=True))\n",
    "    print(save_tagged_sentence(all_path, tagged_tr_semcor_r, 'all_rouge', d_senses_semcor,mode='a', lemma=lemma_opt, sel_verbs=True))\n",
    "    np.save(all_path / 'all_rouge.npy', np.concatenate((rouge_omsti.take(filtered_ids_omsti_r), rouge_semcor.take(filtered_ids_semcor_r))))\n",
    "\n",
    "    np.random.seed(SEED + 10)\n",
    "    ix_random_omsti = np.random.choice(len(tagged_tr_omsti_r), len(filtered_ids_omsti_r), replace=False)\n",
    "    np.random.seed(SEED + 10)\n",
    "    ix_random_semcor = np.random.choice(len(tagged_tr_semcor_r), len(filtered_ids_semcor_r), replace=False)\n",
    "    np.save(all_path / 'all_rouge_random.npy', np.concatenate((rouge_omsti.take(ix_random_omsti), rouge_semcor.take(ix_random_semcor))))\n",
    "    save_tagged_sentence(all_path, np.array(tagged_tr_omsti)[ix_random_omsti], 'all_rouge_random', d_senses_omsti, lemma=lemma_opt, sel_verbs=True)\n",
    "    save_tagged_sentence(all_path, np.array(tagged_tr_semcor)[ix_random_semcor], 'all_rouge_random', d_senses_semcor, mode='a', lemma=lemma_opt, sel_verbs=True)\n",
    "        \n",
    "    print('Saving bleu')\n",
    "    print(save_tagged_sentence(all_path, tagged_tr_omsti_b, 'all_bleu', d_senses_omsti, lemma=lemma_opt, sel_verbs=True))\n",
    "    print(save_tagged_sentence(all_path, tagged_tr_semcor_b, 'all_bleu', d_senses_semcor, mode='a', lemma=lemma_opt, sel_verbs=True))\n",
    "    np.save(all_path / 'all_bleu.npy', np.concatenate((bleu_omsti.take(filtered_ids_omsti_b), bleu_semcor.take(filtered_ids_semcor_b))))\n",
    "    \n",
    "    np.random.seed(SEED + 20)\n",
    "    ix_random_omsti = np.random.choice(len(tagged_tr_omsti_b), len(filtered_ids_omsti_b), replace=False)\n",
    "    np.random.seed(SEED + 20)\n",
    "    ix_random_semcor = np.random.choice(len(tagged_tr_semcor_b), len(filtered_ids_semcor_b), replace=False)\n",
    "    np.save(all_path, np.concatenate((bleu_omsti.take(ix_random_omsti), bleu_semcor.take(ix_random_semcor))))\n",
    "    save_tagged_sentence(all_path, np.array(tagged_tr_omsti)[ix_random_omsti], 'all_bleu_random', d_senses_omsti, lemma=lemma_opt, sel_verbs=True)\n",
    "    save_tagged_sentence(all_path, np.array(tagged_tr_semcor)[ix_random_semcor], 'all_bleu_random', d_senses_semcor, mode='a', lemma=lemma_opt, sel_verbs=True)\n",
    "    \n",
    "    print('Saving rouge and bleu')\n",
    "    print(save_tagged_sentence(all_path, tagged_tr_omsti_r_b, 'all_f1', d_senses_omsti, lemma=lemma_opt, sel_verbs=True))\n",
    "    print(save_tagged_sentence(all_path, tagged_tr_semcor_r_b, 'all_f1', d_senses_semcor, mode='a', lemma=lemma_opt, sel_verbs=True))\n",
    "    np.save(all_path / 'all_f1.npy', np.concatenate((f1_omsti.take(filtered_ids_omsti_r_b), f1_semcor.take(filtered_ids_semcor_r_b))))\n",
    "    \n",
    "    np.random.seed(SEED + 30)\n",
    "    ix_random_omsti = np.random.choice(len(tagged_tr_omsti_r_b), len(filtered_ids_omsti_r_b), replace=False)\n",
    "    np.random.seed(SEED + 30)\n",
    "    ix_random_semcor = np.random.choice(len(tagged_tr_semcor_r_b), len(filtered_ids_semcor_r_b), replace=False)\n",
    "    np.save(all_path, np.concatenate((f1_omsti.take(ix_random_omsti), f1_semcor.take(ix_random_semcor))))\n",
    "    save_tagged_sentence(all_path, np.array(tagged_tr_omsti)[ix_random_omsti], 'all_f1_random', d_senses_omsti, lemma=lemma_opt, sel_verbs=True)\n",
    "    save_tagged_sentence(all_path, np.array(tagged_tr_semcor)[ix_random_semcor], 'all_f1_random', d_senses_semcor, mode='a', lemma=lemma_opt, sel_verbs=True)\n",
    "\n",
    "    print('Saving meteor')\n",
    "    print(save_tagged_sentence(all_path, tagged_tr_omsti_m, 'all_meteor', d_senses_omsti, lemma=lemma_opt, sel_verbs=True))\n",
    "    print(save_tagged_sentence(all_path, tagged_tr_semcor_m, 'all_meteor', d_senses_semcor, mode='a', lemma=lemma_opt, sel_verbs=True))\n",
    "    np.save(all_path / 'all_meteor.npy', np.concatenate((meteor_omsti.take(filtered_ids_omsti_m), meteor_semcor.take(filtered_ids_semcor_m))))\n",
    "    \n",
    "    np.random.seed(SEED + 30)\n",
    "    ix_random_omsti = np.random.choice(len(tagged_tr_omsti_m), len(filtered_ids_omsti_m), replace=False)\n",
    "    np.random.seed(SEED + 30)\n",
    "    ix_random_semcor = np.random.choice(len(tagged_tr_semcor_m), len(filtered_ids_semcor_m), replace=False)\n",
    "    np.save(all_path, np.concatenate((meteor_omsti.take(ix_random_omsti), meteor_semcor.take(ix_random_semcor))))\n",
    "    save_tagged_sentence(all_path, np.array(tagged_tr_omsti)[ix_random_omsti], 'all_meteor_random', d_senses_omsti, lemma=lemma_opt, sel_verbs=True)\n",
    "    save_tagged_sentence(all_path, np.array(tagged_tr_semcor)[ix_random_semcor], 'all_meteor_random', d_senses_semcor, mode='a', lemma=lemma_opt, sel_verbs=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7298912549585453"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
