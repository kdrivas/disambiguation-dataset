{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You provided a validation dataset but patience was set to None, meaning that early stopping is disabled\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from typing import Iterator, List, Dict\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from allennlp.data.dataset_readers.seq2seq import Seq2SeqDatasetReader\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.data.fields import TextField, IndexField\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.data import Instance\n",
    "\n",
    "from allennlp.data.tokenizers.character_tokenizer import CharacterTokenizer\n",
    "from allennlp.data.tokenizers.word_tokenizer import WordTokenizer\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.nn.activations import Activation\n",
    "from allennlp.models.encoder_decoders.simple_seq2seq import SimpleSeq2Seq\n",
    "from allennlp.models import DecomposableAttention\n",
    "from allennlp.modules.attention import LinearAttention, BilinearAttention, DotProductAttention\n",
    "from allennlp.modules.seq2seq_encoders import PytorchSeq2SeqWrapper, StackedSelfAttentionEncoder\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.predictors import SimpleSeq2SeqPredictor\n",
    "from allennlp.training.trainer import Trainer\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "\n",
    "from src.data import prepare_data\n",
    "import numpy as np\n",
    "import os \n",
    "from pathlib import Path\n",
    "\n",
    "EN_EMBEDDING_DIM = 300\n",
    "ZH_EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 256\n",
    "CUDA_DEVICE = 0\n",
    "\n",
    "def evaluate_acc(predictor, test_dataset, pairs_test, selected_synsets, senses_per_sentence, report=False, verbose=False):\n",
    "    dict_pt_verbs = {'tratar_tag': {'total_in_ambiguous': 0, 'total_out_ambiguous': 0, 'hint': 0},\\\n",
    "            'estabelecer_tag': {'total_in_ambiguous': 0, 'total_out_ambiguous': 0, 'hint': 0},\\\n",
    "            'marcar_tag': {'total_in_ambiguous': 0, 'total_out_ambiguous': 0, 'hint': 0},\\\n",
    "            'vir_tag': {'total_in_ambiguous': 0, 'total_out_ambiguous': 0, 'hint': 0},\\\n",
    "            'colocar_tag': {'total_in_ambiguous': 0, 'total_out_ambiguous': 0, 'hint': 0},\\\n",
    "            'fechar_tag': {'total_in_ambiguous': 0, 'total_out_ambiguous': 0, 'hint': 0},\\\n",
    "            'dar_tag': {'total_in_ambiguous': 0, 'total_out_ambiguous': 0, 'hint': 0},\\\n",
    "            'cair_tag': {'total_in_ambiguous': 0, 'total_out_ambiguous': 0, 'hint': 0},\\\n",
    "            'encontrar_tag': {'total_in_ambiguous': 0, 'total_out_ambiguous': 0, 'hint': 0},\\\n",
    "            'registrar_tag': {'total_in_ambiguous': 0, 'total_out_ambiguous': 0, 'hint': 0},\\\n",
    "            'levar_tag': {'total_in_ambiguous': 0, 'total_out_ambiguous': 0, 'hint': 0},\\\n",
    "            'receber_tag': {'total_in_ambiguous': 0, 'total_out_ambiguous': 0, 'hint': 0},\\\n",
    "            'apresentar_tag': {'total_in_ambiguous': 0, 'total_out_ambiguous': 0, 'hint': 0},\\\n",
    "            'passar_tag': {'total_in_ambiguous': 0, 'total_out_ambiguous': 0, 'hint': 0},\\\n",
    "            'deixar_tag': {'total_in_ambiguous': 0, 'total_out_ambiguous': 0, 'hint': 0},\\\n",
    "            'chegar_tag': {'total_in_ambiguous': 0, 'total_out_ambiguous': 0, 'hint': 0},\\\n",
    "            'ficar_tag': {'total_in_ambiguous': 0, 'total_out_ambiguous': 0, 'hint': 0},\\\n",
    "            'fazer_tag': {'total_in_ambiguous': 0, 'total_out_ambiguous': 0, 'hint': 0},\\\n",
    "            'ter_tag': {'total_in_ambiguous': 0, 'total_out_ambiguous': 0, 'hint': 0},\\\n",
    "            'ser_tag': {'total_in_ambiguous': 0, 'total_out_ambiguous': 0, 'hint': 0}}\n",
    "        \n",
    "    hint = 0\n",
    "    total_prec = 0\n",
    "    total_reca = 0\n",
    "\n",
    "    for ix, instance in enumerate(test_dataset):\n",
    "        sentence = pairs_test[ix][0].lower()\n",
    "        senses = senses_per_sentence[ix]\n",
    "        if len(senses) == 0:\n",
    "            continue\n",
    "        output_words = predictor.predict_instance(instance)['predicted_tokens']\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        for pos, sense in senses:\n",
    "            if len(output_words) > pos:  \n",
    "                pred = output_words[pos]\n",
    "                if pred in selected_synsets:\n",
    "                    dict_pt_verbs[sentence.split()[pos]]['total_out_ambiguous'] += 1\n",
    "                    total_prec += 1\n",
    "                    if sense == pred:\n",
    "                        dict_pt_verbs[sentence.split()[pos]]['hint'] += 1\n",
    "                        hint += 1\n",
    "\n",
    "            total_reca += 1\n",
    "            dict_pt_verbs[sentence.split()[pos]]['total_in_ambiguous'] += 1\n",
    "            \n",
    "        if verbose:\n",
    "            print('-O-')\n",
    "\n",
    "    precision = (hint / total_prec) if hint else 0\n",
    "    recall = (hint / total_reca) if hint else 0\n",
    "    f1 = (2 * precision * recall / (precision + recall)) if hint else 0\n",
    "    \n",
    "    if report:\n",
    "        return f1, precision, recall, dict_pt_verbs\n",
    "    else:\n",
    "        return f1\n",
    "\n",
    "def main(name_file='all_f1', train_dir='all', test_dir='test', dir_files='data/disambiguation/', dir_results='results/', max_length=120, cuda_id=0, cuda=True, n_epochs=9, seed=0):\n",
    "    \n",
    "    dir_train = os.path.join(dir_files, train_dir)\n",
    "    dir_test = os.path.join(dir_files, test_dir)\n",
    "    dir_results = os.path.join(dir_results, train_dir, name_file)\n",
    "    os.makedirs(dir_results, exist_ok=True)\n",
    "    \n",
    "    input_lang, output_lang, pairs_train, pairs_test, senses_per_sentence = prepare_data(name_file, 'verbs_selected_lemma', max_length=max_length, dir_train=dir_train, dir_test=dir$\n",
    "    selected_synsets = np.load(os.path.join(dir_files, 'selected_synsets.npy'))\n",
    "\n",
    "    reader = Seq2SeqDatasetReader(\n",
    "        source_tokenizer=WordTokenizer(),\n",
    "        target_tokenizer=WordTokenizer(),\n",
    "        source_token_indexers={'tokens': SingleIdTokenIndexer()},\n",
    "        target_token_indexers={'tokens': SingleIdTokenIndexer(namespace='target_tokens')})\n",
    "    train_dataset = reader.read(os.path.join(dir_train, name_file + '.tsv'))\n",
    "    validation_dataset = reader.read(os.path.join(dir_test, 'verbs_selected_lemma.tsv'))\n",
    "\n",
    "    vocab = Vocabulary.from_instances(train_dataset + validation_dataset,\n",
    "                                      min_count={'tokens': 3, 'target_tokens': 3})\n",
    "\n",
    "    en_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                             embedding_dim=EN_EMBEDDING_DIM)\n",
    "\n",
    "    encoder = StackedSelfAttentionEncoder(input_dim=EN_EMBEDDING_DIM, hidden_dim=HIDDEN_DIM, projection_dim=128, feedforward_hidden_dim=128, num_layers=1, num_attention_heads=8)\n",
    "\n",
    "    source_embedder = BasicTextFieldEmbedder({\"tokens\": en_embedding})\n",
    "    # attention = LinearAttention(HIDDEN_DIM, HIDDEN_DIM, activation=Activation.by_name('tanh')())\n",
    "    # attention = BilinearAttention(HIDDEN_DIM, HIDDEN_DIM)\n",
    "    attention = DotProductAttention()\n",
    "\n",
    "    max_decoding_steps = 100   # TODO: make this variable\n",
    "    model = SimpleSeq2Seq(vocab, source_embedder, encoder, max_decoding_steps,\n",
    "                          target_embedding_dim=ZH_EMBEDDING_DIM,\n",
    "                          target_namespace='target_tokens',\n",
    "                          attention=attention,\n",
    "                          beam_size=8,\n",
    "                          use_bleu=True).cuda()\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    iterator = BucketIterator(batch_size=32, sorting_keys=[(\"source_tokens\", \"num_tokens\")])\n",
    "\n",
    "    iterator.index_with(vocab)\n",
    "\n",
    "    trainer = Trainer(model=model,\n",
    "                      optimizer=optimizer,\n",
    "                      iterator=iterator,\n",
    "                      train_dataset=train_dataset,\n",
    "                      validation_dataset=validation_dataset,\n",
    "                      patience=7,\n",
    "                      num_epochs=25,\n",
    "                      cuda_device=cuda_id)\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    trainer = Trainer(model=model,\n",
    "                      optimizer=optimizer,\n",
    "                      iterator=iterator,\n",
    "                      train_dataset=train_dataset,\n",
    "                      validation_dataset=validation_dataset,\n",
    "                      patience=7,\n",
    "                      num_epochs=1,\n",
    "                      cuda_device=cuda_id)\n",
    "\n",
    "    best_metric = 0\n",
    "    metrics = []\n",
    "    for i in range(0, 40):\n",
    "        print('Epoch: {}'.format(i))\n",
    "        trainer.train()\n",
    "\n",
    "        predictor = SimpleSeq2SeqPredictor(model, reader)\n",
    "        if True:\n",
    "            metric = evaluate_acc(predictor, validation_dataset, pairs_test, selected_synsets, senses_per_sentence, report=False, verbose=False)\n",
    "            metrics.append(metric)\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                with open(os.path.join(dir_results, \"allen.th\"), 'wb') as f:\n",
    "                    torch.save(model.state_dict(), f)\n",
    "                print('-----best----', best_metric)\n",
    "\n",
    "    np.save(os.path.join(dir_results, 'metrics_allen.npy'), metrics)\n",
    "    with open(os.path.join(dir_results, \"allen.th\"), 'rb') as f:\n",
    "        model.load_state_dict(torch.load(f))\n",
    "    f1, precision, recall, report = evaluate_acc(predictor, validation_dataset, pairs_test, selected_synsets, senses_per_sentence, report=True, verbose=False)\n",
    "    print('f1 score:', f1, 'precision:', precision, 'recall:', recall)\n",
    "\n",
    "    res = get_stats(report, pairs_train, pairs_test)\n",
    "    res.to_csv(f'{dir_results}/report_allen.csv')\n",
    "                                                                                         \n",
    "if __name__ == '__main__':\n",
    "    fire.Fire(main)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
