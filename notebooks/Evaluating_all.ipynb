{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "os.chdir('../')\n",
    "\n",
    "from src.data import read_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_sentences(tagged_tr, d_senses, lemma=False, sel_verbs=True):\n",
    "    tagged_sense_sent = []\n",
    "    input_sense_sent = []\n",
    "    orig_sent = []\n",
    "    cont = 0\n",
    "    a = 0\n",
    "    for instance in tagged_tr:\n",
    "        temp_o = []\n",
    "        temp_i = []\n",
    "        for ix, word in enumerate(instance[1].split()):\n",
    "            if instance[0][ix] != 'no_instance':\n",
    "                if len(wordnet_senses) != 0:\n",
    "                    if sel_verbs and d_senses[instance[0][ix]][0] in wordnet_senses and (nlp_pt(word)[0].lemma_ in PT_DICT_VERBS or wordnet_senses[d_senses[instance[0][ix]][0]] in SELECTED_SYNSETS):\n",
    "                        temp_o.append(wordnet_senses[d_senses[instance[0][ix]][0]])\n",
    "                        if lemma:\n",
    "                            d = nlp_pt(word)\n",
    "                            lemma = d[0].lemma_ \n",
    "                            if lemma == 'deixar-me':\n",
    "                                lemma = 'deixar'\n",
    "                            temp_i.append(lemma + '_tag')\n",
    "                        else:\n",
    "                            temp_i.append(word)\n",
    "                        a += 1\n",
    "                    elif not sel_verbs and d_senses[instance[0][ix]][0] in wordnet_senses:\n",
    "                        temp_o.append(wordnet_senses[d_senses[instance[0][ix]][0]])\n",
    "                        if lemma:\n",
    "                            d = nlp_pt(word)\n",
    "                            lemma = d[0].lemma_ \n",
    "                            if lemma == 'deixar-me':\n",
    "                                lemma = 'deixar'\n",
    "                            temp_i.append(lemma + '_tag')\n",
    "                        else:\n",
    "                            temp_i.append(word)\n",
    "                        a += 1\n",
    "                    else:\n",
    "                        cont += 1\n",
    "                        temp_o.append(word)\n",
    "                        temp_i.append(word)\n",
    "                else:\n",
    "                    temp_o.append(d_senses[instance[0][ix]][0])\n",
    "                    temp_i.append(word)\n",
    "            else:\n",
    "                temp_o.append(word)\n",
    "                temp_i.append(word)\n",
    "\n",
    "        input_sense_sent.append(' '.join(temp_i))\n",
    "        tagged_sense_sent.append(' '.join(temp_o))\n",
    "        orig_sent.append(instance[2])\n",
    "    \n",
    "    return tagged_sense_sent, input_sense_sent, orig_sent\n",
    "\n",
    "def save_tagged_sentence(dir_path, name_file, tagged_sense_sent, input_sense_sent, orig_sent, mode='w'):\n",
    "    \n",
    "    with open(dir_path / f'{name_file}_out.txt', mode) as file:\n",
    "        for sent in tagged_sense_sent:\n",
    "            file.write(sent + '\\n')\n",
    "    file.close()\n",
    "    \n",
    "    with open(dir_path / f'{name_file}_in.txt', mode) as file:\n",
    "        for sent in input_sense_sent:\n",
    "            file.write(sent + '\\n')\n",
    "    file.close() \n",
    "    \n",
    "    with open(dir_path / f'{name_file}_orig.txt', mode) as file:\n",
    "        for sent in orig_sent:\n",
    "            file.write(sent + '\\n')\n",
    "    file.close()     \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PT_VERBS = ['tratar', 'estabelecer', 'marcar', 'vir', 'colocar',\\\n",
    "           'fechar', 'dar', 'cair', 'encontrar', 'registrar',\\\n",
    "           'levar', 'receber', 'apresentar', 'passar', 'deixar',\\\n",
    "           'chegar', 'ficar', 'fazer', 'ter', 'ser']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_SYNSETS = np.load(Path.cwd() / 'data/disambiguation/selected_synsets.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PT_DICT_VERBS ={\n",
    "    'tratar': ['deal with', 'treat', 'process', 'deal', 'manage', 'do', 'attend', 'look after', 'cherrish', 'misuse', 'size'],\\\n",
    "    'estabelecer': ['establish', 'set', 'fix', 'lay down', 'make', 'settle', 'determine', 'presribe', 'impose', 'enter into', 'stipulate', 'organize', 'seat'],\\\n",
    "    'marcar': ['mark', 'brand', 'dial', 'book', 'stamp', 'show', 'read', 'define', 'trace', 'earmark', 'reserve', 'feature', 'signilize', 'scribe'],\\\n",
    "    'vir': ['come', 'arrive', 'come on', 'come up with'],\\\n",
    "    'colocar': ['put', 'lay', 'set', 'place', 'post', 'pose', 'stick', 'plant', 'dispose', 'collocate', 'posit', 'bestow', 'pitch', 'clap', 'ship', 'placatory'],\\\n",
    "    'fechar': ['close', 'shut', 'seal', 'turn off', 'pin down', 'fasten', 'occlude', 'shut in', 'box', 'impount', 'bar', 'berate', 'stop up', 'mure', 'stopple', 'rail', 'pen', 'inmure'],\\\n",
    "    'dar': ['to give', 'give', 'impart', 'provide', 'render', 'afford', 'yield', 'allow', 'hand', 'deal', 'administer', 'give in', 'gift', 'confer', 'inflict', 'handsel', 'accord'],\\\n",
    "    'cair': ['fall', 'go down', 'drop', 'sink', 'collapse', 'founder', 'topple', 'lapse', 'sleet', 'keel over', 'prey', 'prostrate', 'pelt', 'plump', 'flump'],\\\n",
    "    'encontrar': ['meet', 'find', 'detect', 'encounter', 'find out', 'discover', 'meet with', 'experience', 'get together', 'impinge', 'hunt up'],\\\n",
    "    'registrar': ['register', 'record', 'read', 'book', 'enroll', 'inscribe', 'enrol', 'list', 'write down', 'set down', 'trace', 'score', 'label', 'matriculate', 'prick down', 'calendar', 'signalize'],\\\n",
    "    'levar': ['take along', 'take', 'carry', 'convey', 'go', 'prompt', 'induce', 'hold', 'charge', 'ravish'],\\\n",
    "    'receber': ['to receive', 'receive', 'welcome', 'get', 'have', 'accept', 'collect', 'meet', 'entertain', 'do', 'reap', 'derive', 'salute'],\\\n",
    "    'apresentar': ['to present', 'present', 'introduce', 'show', 'exhibit', 'lodge', 'produce', 'put', 'bring forward', 'come up with', 'represent', 'bring up', 'render'],\\\n",
    "    'passar': ['pass', 'spend', 'hand', 'go', 'go by', 'elapse', 'slip away', 'come', 'transfer', 'give in'],\\\n",
    "    'deixar': ['leave', 'let', 'have', 'quit', 'let go', 'depart', 'go away', 'drop off', 'leave out'],\\\n",
    "    'chegar': ['to arrive', 'arrive', ' get in', 'come', 'achieve', 'land', 'get around', 'turn up'],\\\n",
    "    'ficar': ['stay', 'bide', 'be', 'remain', 'go', 'continue', 'keep', 'come'],\\\n",
    "    'fazer': ['do', 'perform', 'make', 'cause', 'create', 'produce', 'render', 'manufacture'],\\\n",
    "    'ter': ['tue', 'have', 'take'],\\\n",
    "    'ser': ['to be', 'be', 'being']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval senses tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_file_semcor = open('data/translation/semcor/en_pt/semcor_or_2.txt', 'r')\n",
    "tr_file_semcor = open('data/translation/semcor/en_pt/semcor_tr_2.txt', 'r')\n",
    "id_file_semcor = open('data/translation/semcor/en_pt/semcor_id_2.txt', 'r')\n",
    "al_file_semcor = open('data/translation/semcor/en_pt/semcor_al_2.txt', 'r')\n",
    "\n",
    "or_file_omsti = open('data/translation/omsti/en_pt/omsti_or.txt', 'r')\n",
    "tr_file_omsti = open('data/translation/omsti/en_pt/omsti_tr.txt', 'r')\n",
    "id_file_omsti = open('data/translation/omsti/en_pt/omsti_id.txt', 'r')\n",
    "al_file_omsti = open('data/translation/omsti/en_pt/omsti_al.txt', 'r')\n",
    "\n",
    "or_file_dev = open('data/translation/dev/en_pt/dev_or.txt', 'r')\n",
    "tr_file_dev = open('data/translation/dev/en_pt/dev_tr.txt', 'r')\n",
    "id_file_dev = open('data/translation/dev/en_pt/dev_id.txt', 'r')\n",
    "al_file_dev = open('data/translation/dev/en_pt/dev_al.txt', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_alignment(chars):\n",
    "    alignment = []\n",
    "    for e in re.split(r'[,|;]', chars):\n",
    "        temp = e.split('-')\n",
    "        range_a = temp[0].split(':')\n",
    "        range_b = temp[1].split(':')\n",
    "        alignment.append([np.arange(int(range_a[0]), int(range_a[0]) + int(range_a[1]) + 1),\\\n",
    "                         np.arange(int(range_b[0]), int(range_b[0]) + int(range_b[1]) + 1)])\n",
    "        \n",
    "    return alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_arrays(or_file, tr_file, id_file, al_file):\n",
    "    tr_ar = []\n",
    "    for sent in tr_file:\n",
    "        tr_ar.append(sent.replace('\\n', ''))\n",
    "        \n",
    "    or_ar = []\n",
    "    for sent in or_file:\n",
    "        or_ar.append(sent.replace('\\n', ''))\n",
    "\n",
    "    id_ar = []\n",
    "    for sent in id_file:\n",
    "        id_ar.append(int(sent.replace('\\n', '')))\n",
    "\n",
    "    al_ar = []\n",
    "    for sent in al_file:\n",
    "        al_ar.append(parse_alignment(sent.replace('\\n', '')))\n",
    "        \n",
    "    return np.array(or_ar), np.array(tr_ar), np.array(id_ar), np.array(al_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_semcor, tr_semcor, id_semcor, al_semcor = load_arrays(or_file_semcor, tr_file_semcor, id_file_semcor, al_file_semcor)\n",
    "or_omsti, tr_omsti, id_omsti, al_omsti = load_arrays(or_file_omsti, tr_file_omsti, id_file_omsti, al_file_omsti)\n",
    "or_dev, tr_dev, id_dev, al_dev = load_arrays(or_file_dev, tr_file_dev, id_file_dev, al_file_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_all = np.concatenate((or_semcor, or_omsti, or_dev), axis=0)\n",
    "tr_all = np.concatenate((tr_semcor, tr_omsti, tr_dev), axis=0)\n",
    "id_all = np.concatenate((id_semcor, id_omsti, id_dev), axis=0)\n",
    "al_all = np.concatenate((al_semcor, al_omsti, al_dev), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37163, 37163, 37163, 37163)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(or_semcor), len(tr_semcor), len(id_semcor), len(al_semcor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40260, 40260, 40260, 40260)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(or_omsti), len(tr_omsti), len(id_omsti), len(al_omsti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145, 145, 145, 145)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(or_dev), len(tr_dev), len(id_dev), len(al_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77568, 77568, 77568, 77568)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(or_all), len(tr_all), len(id_all), len(al_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_file_semcor.close()\n",
    "tr_file_semcor.close()\n",
    "id_file_semcor.close()\n",
    "al_file_semcor.close()\n",
    "\n",
    "or_file_omsti.close()\n",
    "tr_file_omsti.close()\n",
    "id_file_omsti.close()\n",
    "al_file_omsti.close()\n",
    "\n",
    "or_file_dev.close()\n",
    "tr_file_dev.close()\n",
    "id_file_dev.close()\n",
    "al_file_dev.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read corpus annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sent_semcor, sent_to_id_semcor, target_sent_semcor = read_sentences(Path.cwd() / 'data/WSD_Evaluation_Framework/Training_Corpora/', 'SemCor', False)\n",
    "input_sent_dev, sent_to_id_dev, target_sent_dev = read_sentences(Path.cwd() / 'data/WSD_Evaluation_Framework/Evaluation_Datasets/', 'ALL', False)\n",
    "input_sent_temp, sent_to_id_temp, target_sent_temp = read_sentences(Path.cwd() / 'data/WSD_Evaluation_Framework/Training_Corpora/', 'SemCor+OMSTI', True)\n",
    "\n",
    "# Filter sentences\n",
    "input_sent_omsti = []\n",
    "sent_to_id_omsti = []\n",
    "target_sent_omsti = []\n",
    "for ix, sent in enumerate(input_sent_temp):\n",
    "    if sent not in input_sent_semcor:\n",
    "        input_sent_omsti.append(sent)\n",
    "        sent_to_id_omsti.append(sent_to_id_temp[ix])\n",
    "        target_sent_omsti.append(target_sent_temp[ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check amount of valid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alignment(pos_word, al):\n",
    "    temp = []\n",
    "    for e in al:\n",
    "        if pos_word in e[1]:\n",
    "            temp.append(e)\n",
    "    \n",
    "    if len(temp):\n",
    "        temp = sorted(temp, key= lambda x: len(x[1]))\n",
    "        temp = temp[0]\n",
    "        \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words_with_verb(sent):\n",
    "    temp_ix = []\n",
    "    temp_cont = []\n",
    "    cont_char = 0\n",
    "    for ix, word in enumerate(sent.split()):\n",
    "        if word in PT_VERBS:\n",
    "            temp_cont.append(cont_char)\n",
    "            temp_ix.append(ix)\n",
    "        cont_char += len(word) + 1\n",
    "    return temp_cont, temp_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tagged_instance(id_arr, al_arr, tr_arr, input_sent_arr, sent_to_id_arr, target_sent_arr):\n",
    "    cont = 0\n",
    "    for ix_tr, ix_input_sent in enumerate(id_arr):\n",
    "        \n",
    "        input_sent = input_sent_arr[ix_input_sent]\n",
    "        sent_to_id = sent_to_id_arr[ix_input_sent]\n",
    "        target_sent = target_sent_arr[ix_input_sent]\n",
    "        \n",
    "        al = al_arr[ix_tr]\n",
    "        tr = tr_arr[ix_tr]\n",
    "        \n",
    "        arr_cont_char, arr_pos_word = count_words_with_verb(tr)\n",
    "        if len(arr_cont_char):\n",
    "            for cont_char, pos_word in zip(arr_cont_char, arr_pos_word):\n",
    "                alingment = get_alignment(cont_char, al)\n",
    "                #print(alingment)\n",
    "                t_or = min(alingment[0])\n",
    "                d_or = max(alingment[0])\n",
    "                t_tr = min(alingment[1])\n",
    "                d_tr = max(alingment[1])\n",
    "\n",
    "                ant_n_words = len(input_sent[:t_or].split())\n",
    "                n_words = len(input_sent[t_or:d_or].split())\n",
    "                for id_sense in sent_to_id[ant_n_words:(ant_n_words+n_words)]:\n",
    "                    if target_sent[id_sense] != 'no_instance':\n",
    "                        cont +=1\n",
    "    \n",
    "    return cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "class WhitespaceTokenizer(object):\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __call__(self, text):\n",
    "        words = text.split(' ')\n",
    "        # All tokens 'own' a subsequent space character in this tokenizer\n",
    "        spaces = [True] * len(words)\n",
    "        return Doc(self.vocab, words=words, spaces=spaces)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp_pt = spacy.load('pt_core_news_sm')\n",
    "nlp_pt.tokenizer = WhitespaceTokenizer(nlp_pt.vocab)\n",
    "\n",
    "nlp_en = spacy.load('en_core_web_sm')\n",
    "nlp_en.tokenizer = WhitespaceTokenizer(nlp_en.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_file_semcor = open('data/translation/semcor/pt_en/semcor_or_2.txt', 'r')\n",
    "tr_file_semcor = open('data/translation/semcor/pt_en/semcor_tr_2.txt', 'r')\n",
    "al_file_semcor = open('data/translation/semcor/pt_en/semcor_al_2.txt', 'r')\n",
    "\n",
    "or_file_omsti = open('data/translation/omsti/pt_en/omsti_or_2.txt', 'r')\n",
    "tr_file_omsti = open('data/translation/omsti/pt_en/omsti_tr_2.txt', 'r')\n",
    "al_file_omsti = open('data/translation/omsti/pt_en/omsti_al_2.txt', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_file_dev = open('data/translation/dev/pt_en/dev_or.txt', 'r')\n",
    "tr_file_dev = open('data/translation/dev/pt_en/dev_tr.txt', 'r')\n",
    "al_file_dev = open('data/translation/dev/pt_en/dev_al.txt', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "bck_or_semcor, bck_tr_semcor, _, bck_al_semcor = load_arrays(or_file_semcor, tr_file_semcor, [], al_file_semcor)\n",
    "bck_or_omsti, bck_tr_omsti, _, bck_al_omsti = load_arrays(or_file_omsti, tr_file_omsti, [], al_file_omsti)\n",
    "bck_or_dev, bck_tr_dev, _, bck_al_dev = load_arrays(or_file_dev, tr_file_dev, [], al_file_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "bck_or_all = np.concatenate((bck_or_semcor, bck_or_omsti, bck_or_dev), axis=0)\n",
    "bck_tr_all = np.concatenate((bck_tr_semcor, bck_tr_omsti, bck_tr_dev), axis=0)\n",
    "bck_al_all = np.concatenate((bck_al_semcor, bck_al_omsti, bck_al_dev), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how long has it been since you reviewed the objectives of your benefit and service program ?'"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "or_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_file_semcor.close()\n",
    "tr_file_semcor.close()\n",
    "al_file_semcor.close()\n",
    "\n",
    "or_file_omsti.close()\n",
    "tr_file_omsti.close()\n",
    "al_file_omsti.close()\n",
    "\n",
    "or_file_dev.close()\n",
    "tr_file_dev.close()\n",
    "al_file_dev.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the next days may show where things are .',\n",
       " 'the next days may show where things stand .')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bck_tr_semcor[-100], or_semcor[-100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('whether as athletes or spectators , when we find ourselves in and participate in international sporting events , we share the brilliance of world-class competition , whether it be in the pain of losing, or the glory of winning , but more importantly , on a willingness of participation .',\n",
       " 'whether as athletes or spectators , when we meet at and participate in international sporting events , we share in the glow of world-class competition , whether it be in the heartbreak of losing or in the glory of winning , but most importantly , in the goodwill of participation .')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bck_tr_omsti[-100], or_omsti[-100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('it is clear that on Thursday , haney mailed the monthly check for separate maintenance to his wife lolly , and wished the stranger could do something about it',\n",
       " 'of course on thursday , haney mailed the monthly check for separate maintenance to his wife lolly , and wished the stranger could do something about her')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bck_tr_dev[-100], or_dev[-100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rouge_per_sentence(input_sentences, trns_sentences):\n",
    "    f1_scores = []\n",
    "    for sent, trns in zip(input_sentences, trns_sentences):\n",
    "        evaluator = rouge.Rouge(metrics=['rouge-n'],\n",
    "                               max_n=2,\n",
    "                               limit_length=True,\n",
    "                               length_limit=100,\n",
    "                               length_limit_type='words',\n",
    "                               alpha=0.5, # Default F1_score\n",
    "                               weight_factor=1.2,\n",
    "                               stemming=True)\n",
    "\n",
    "        all_hypothesis = [trns]\n",
    "        all_references = [sent]\n",
    "\n",
    "        scores = evaluator.get_scores(all_hypothesis, all_references)\n",
    "        for metric, results in sorted(scores.items(), key=lambda x: x[0]):\n",
    "            if metric == 'rouge-2':\n",
    "                f1_scores.append(results['f'])\n",
    "\n",
    "    return np.array(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_all = get_rouge_per_sentence(or_all, bck_tr_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval Bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "def get_bleu_per_sentence(input_sentences, trns_sentences):\n",
    "    bleu_scores = []\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    for sent, trns in zip(input_sentences, trns_sentences):\n",
    "        hypothesis = trns.split() \n",
    "        reference = sent.split()\n",
    "        \n",
    "        references = [reference]\n",
    "        bleu = nltk.translate.bleu_score.sentence_bleu(references, hypothesis, smoothing_function=smoothie)\n",
    "        \n",
    "        bleu_scores.append(bleu)\n",
    "\n",
    "    return np.array(bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_all = get_bleu_per_sentence(or_all, bck_tr_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1_per_sentence(bleu, rouge):\n",
    "    \n",
    "    f1 = []\n",
    "    for b, r in zip(bleu, rouge):\n",
    "        if b != 0 or r != 0:\n",
    "            f1.append(2 * (b * r) / (b + r))\n",
    "        else:\n",
    "            f1.append(0)\n",
    "        \n",
    "    return np.array(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_all = get_f1_per_sentence(bleu_all, rouge_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval meteor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meteor_per_sentence(name):\n",
    "    \n",
    "    meteor = []\n",
    "    with open(f'data/meteor_output/{name}.txt') as f:\n",
    "        for line in f.read().split('\\n'):\n",
    "            if 'Segment' in line and 'score' in line:\n",
    "                meteor.append(float(line.split(':')[1]))\n",
    "        \n",
    "    return np.array(meteor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteor_semcor = get_meteor_per_sentence('semcor')\n",
    "meteor_omsti = get_meteor_per_sentence('omsti')\n",
    "meteor_dev = get_meteor_per_sentence('dev')\n",
    "meteor_all = np.concatenate((meteor_semcor, meteor_omsti, meteor_dev), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer\n",
    "import re, string, unicodedata\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def normalize_text(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "import scipy \n",
    "\n",
    "def calc_vector(text, model):\n",
    "    item = html.unescape(text)\n",
    "    gen_doc = [w for w in word_tokenize(item)]\n",
    "    normalized = normalize_text(gen_doc)\n",
    "    \n",
    "    model.random.seed(SEED)\n",
    "    return model.infer_vector(normalized, epochs=6)\n",
    "\n",
    "def similarity(text1, text2, model):\n",
    "    vec1 = calc_vector(text1, model)\n",
    "    vec2 = calc_vector(text2, model)    \n",
    "    return 1-scipy.spatial.distance.cosine(vec1, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim_per_sentence(or_sents, bck_sents):\n",
    "    \n",
    "    sims = []\n",
    "    for or_s, bck_s in zip(or_sents, bck_sents):\n",
    "        sims.append(similarity(or_s, bck_s, model))\n",
    "        \n",
    "    return np.array(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "model = Doc2Vec.load('data/vectors/enwiki_doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_semcor = get_sim_per_sentence(or_semcor, bck_tr_semcor)\n",
    "sim_omsti = get_sim_per_sentence(or_omsti, bck_tr_omsti)\n",
    "sim_dev = get_sim_per_sentence(or_dev, bck_tr_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_all = np.concatenate((sim_semcor, sim_omsti, sim_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77568"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(f1_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rouge_omsti = np.load(Path.cwd() / 'data/disambiguation/all_less_mean' / 'rouge.npy')[:len(id_omsti)]\n",
    "#bleu_omsti = np.load(Path.cwd() / 'data/disambiguation/all_less_mean' / 'bleu.npy')[:len(id_omsti)]\n",
    "#f1_omsti = np.load(Path.cwd() / 'data/disambiguation/all_less_mean' / 'f1.npy')[:len(id_omsti)]\n",
    "#meteor_omsti = np.load(Path.cwd() / 'data/disambiguation/all_less_mean' / 'meteor.npy')[:len(id_omsti)]\n",
    "#sim_omsti = np.load(Path.cwd() / 'data/disambiguation/all_less_mean' / 'sim.npy')[:len(id_omsti)]\n",
    "\n",
    "#rouge_semcor = np.load(Path.cwd() / 'data/disambiguation/all_less_mean' / 'rouge.npy')[len(id_omsti):]\n",
    "#bleu_semcor = np.load(Path.cwd() / 'data/disambiguation/all_less_mean' / 'bleu.npy')[len(id_omsti):]\n",
    "#f1_semcor = np.load(Path.cwd() / 'data/disambiguation/all_less_mean' / 'f1.npy')[len(id_omsti):]\n",
    "#meteor_semcor = np.load(Path.cwd() / 'data/disambiguation/all_less_mean' / 'meteor.npy')[len(id_omsti):]\n",
    "#sim_semcor = np.load(Path.cwd() / 'data/disambiguation/all_less_mean' / 'sim.npy')[len(id_omsti):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(sent):\n",
    "    temp_ix = []\n",
    "    temp_cont = []\n",
    "    cont_char = 0\n",
    "    for ix, word in enumerate(sent.split()):\n",
    "        temp_cont.append(cont_char)\n",
    "        temp_ix.append(ix)\n",
    "        \n",
    "        cont_char += len(word) + 1\n",
    "        \n",
    "    return temp_cont, temp_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_word(tr_word, orig_words):\n",
    "    if tr_word in PT_DICT_VERBS:\n",
    "        orig_options = PT_DICT_VERBS[tr_word]\n",
    "        for orig_word in orig_words:\n",
    "            if orig_word in orig_options:\n",
    "                return True\n",
    "                \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tags(orig_sentences, trns_sentences): \n",
    "    tags_tr = []\n",
    "    for sent in (trns_sentences):\n",
    "        if len(sent):\n",
    "            tag_tr = []\n",
    "            temp = nlp_pt(str(sent).strip())\n",
    "            for token in temp:\n",
    "                tag_tr.append([token.text, token.pos_])\n",
    "            tags_tr.append(tag_tr)\n",
    "        else:\n",
    "            tags_tr.append(['invalid', 'invalid'])\n",
    "    \n",
    "    tags_or = []\n",
    "    for sent in (orig_sentences):\n",
    "        if len(sent):\n",
    "            tag_or = []\n",
    "            temp = nlp_en(str(sent).strip())\n",
    "            for token in temp:\n",
    "                tag_or.append([token.text, token.pos_])\n",
    "            tags_or.append(tag_or)\n",
    "        else:\n",
    "            tags_or.append(['invalid', 'invalid'])\n",
    "        \n",
    "    return tags_or, tags_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagged_translations(selected_id_tr, id_arr, al_arr, tr_arr, input_sent_arr, sent_to_id_arr, target_sent_arr, pos_or, pos_tr):\n",
    "    cont = 0\n",
    "    tagged_tr = []\n",
    "    \n",
    "    for ix_tr, ix_input_sent in zip(selected_id_tr, id_arr.take(selected_id_tr)):\n",
    "        input_sent = input_sent_arr[ix_input_sent]\n",
    "        sent_to_id = sent_to_id_arr[ix_input_sent]\n",
    "        target_sent = target_sent_arr[ix_input_sent]\n",
    "        \n",
    "        al = al_arr[ix_tr]\n",
    "        tr = tr_arr[ix_tr]\n",
    "        \n",
    "        arr_cont_char, arr_pos_word = count_words(tr)\n",
    "        if len(arr_cont_char):\n",
    "            \n",
    "            tags_tr = pos_tr[ix_tr]\n",
    "            tags_or = pos_or[ix_input_sent]\n",
    "            if verbose:\n",
    "                print()\n",
    "                print('sentence orig: ', input_sent)\n",
    "                print('sentence translated: ', tr)\n",
    "            temp = []\n",
    "            for cont_char, pos_word in zip(arr_cont_char, arr_pos_word):\n",
    "                alingment = get_alignment(cont_char, al)\n",
    "                if len(alingment):\n",
    "                    t_or = min(alingment[0])\n",
    "                    d_or = max(alingment[0])\n",
    "                    t_tr = min(alingment[1])\n",
    "                    d_tr = max(alingment[1])\n",
    "\n",
    "                    ant_n_words_or = len(input_sent[:t_or].split())\n",
    "                    n_words_or = len(input_sent[t_or:d_or].split())\n",
    "                    \n",
    "                    ant_n_words_tr = len(tr[:t_tr].split())\n",
    "                    n_words_tr = len(tr[t_tr:d_tr].split())\n",
    "                    \n",
    "                    input_sent_tokens = input_sent.split()[ant_n_words_or:(ant_n_words_or+n_words_or)]\n",
    "                    trns_sent_tokens = tr.split()[ant_n_words_tr:(ant_n_words_tr+n_words_tr)]\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print()\n",
    "                        print('translated word: ', tr.split()[pos_word])\n",
    "                        print('senses: ', np.array(target_sent).take(sent_to_id[ant_n_words_or:(ant_n_words_or+n_words_or)]))\n",
    "                        print('token input original: ', input_sent_tokens)\n",
    "                        print('token translations: ', tr.split()[ant_n_words_tr:(ant_n_words_tr+n_words_tr)])\n",
    "                        print('pos original: ', tags_tr[pos_word])\n",
    "                        print('pos translation: ', tags_or[ant_n_words_or:(ant_n_words_or+n_words_or)])\n",
    "                    if len(input_sent_tokens) == 1:   \n",
    "                        \n",
    "                        tag_or = tags_or[ant_n_words_or:(ant_n_words_or+n_words_or)][0]\n",
    "                        tag_tr = tags_tr[pos_word]\n",
    "                        id_sense = sent_to_id[ant_n_words_or:(ant_n_words_or+n_words_or)][0]\n",
    "                        if verbose:\n",
    "                            print('token 1: ', target_sent[id_sense])\n",
    "                        if target_sent[id_sense] != 'no_instance' and tag_or[1] == tag_tr[1]: \n",
    "                            temp.append(target_sent[id_sense])\n",
    "                        else:\n",
    "                            temp.append('no_instance') \n",
    "\n",
    "                    elif len(input_sent_tokens) <= 4 and len(trns_sent_tokens) <= 5 and check_word(tr.split()[pos_word], input_sent[t_or:d_or].split()):\n",
    "                        is_ambiguous = False\n",
    "\n",
    "                        for id_sense, word in zip(sent_to_id[ant_n_words_or:(ant_n_words_or+n_words_or)], input_sent_tokens):\n",
    "                            if target_sent[id_sense] != 'no_instance' and word in PT_DICT_VERBS[tr.split()[pos_word]]:\n",
    "                                is_ambiguous = True\n",
    "                                break\n",
    "                        \n",
    "                        if is_ambiguous:\n",
    "                            if verbose:\n",
    "                                print('token check_word: ', target_sent[id_sense])\n",
    "                            temp.append(target_sent[id_sense])\n",
    "                        else:\n",
    "                            temp.append('no_instance')\n",
    "                            \n",
    "                    elif len(input_sent_tokens) <= 4 and len(trns_sent_tokens) <= 4:   \n",
    "                        \n",
    "                        is_ambiguous = False\n",
    "                        tag_tr = tags_tr[pos_word]\n",
    "                        \n",
    "                        tag_cntr_or = Counter([tag[1] for tag in tags_or[ant_n_words_or:(ant_n_words_or+n_words_or)]])\n",
    "                        tag_cntr_tr = Counter([tag[1] for tag in tags_tr[ant_n_words_tr:(ant_n_words_tr+n_words_tr)]])\n",
    "\n",
    "                        for id_sense, tag_or in zip(sent_to_id[ant_n_words_or:(ant_n_words_or+n_words_or)], tags_or[ant_n_words_or:(ant_n_words_or+n_words_or)]):\n",
    "                            if target_sent[id_sense] != 'no_instance' and tag_or[1] == tag_tr[1] and tag_cntr_tr[tag_tr[1]] == 1 and tag_cntr_or[tag_or[1]] == 1: \n",
    "                                is_ambiguous = True\n",
    "                                temp_id = target_sent[id_sense]\n",
    "                                if verbose:\n",
    "                                    print('sentido encontrado: ', temp_id)\n",
    "                                break\n",
    "                                \n",
    "                        if is_ambiguous:\n",
    "                            temp.append(temp_id)\n",
    "                        else:\n",
    "                            temp.append('no_instance')\n",
    "\n",
    "                    else:\n",
    "                        temp.append('no_instance')\n",
    "                else:\n",
    "                    temp.append('no_instance')\n",
    "            \n",
    "            tagged_tr.append([temp, tr, input_sent])\n",
    "    \n",
    "    return tagged_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verbs(sent):\n",
    "    temp_cont = []\n",
    "    for ix, word in enumerate(sent.split()):\n",
    "        if nlp_pt(word)[0].lemma_ in PT_VERBS:\n",
    "            temp_cont.append(ix)\n",
    "    return temp_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_instances_with_verbs(tagged_tr):\n",
    "    cont_d = []\n",
    "    cont_n = []\n",
    "    for ix_instance, instance in enumerate(tagged_tr):\n",
    "        ambiguous=False\n",
    "        for ix_word, word in enumerate(instance[1].split()):\n",
    "            if instance[0][ix_word] != 'no_instance' and nlp_pt(word)[0].lemma_ in PT_VERBS:\n",
    "                cont_d.append(ix_instance)\n",
    "                ambiguous=True\n",
    "                break\n",
    "        if not ambiguous:\n",
    "            cont_n.append(ix_instance)\n",
    "                \n",
    "    return cont_d, cont_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_index(metrics, sims, porc, p=''):\n",
    "    \n",
    "    filtered_ids = []\n",
    "    if p == 'm':\n",
    "        threshold_metric = np.mean(metrics)\n",
    "    else:\n",
    "        threshold_metric = np.mean(metrics)\n",
    "        \n",
    "    threshold_sim = np.mean(sims) + np.std(sims)\n",
    "    ordered_indexes = np.argsort(-metrics)\n",
    "    filtered_ids = ordered_indexes[:int(len(ordered_indexes) * porc)]\n",
    "    #for ix, (metric, sim) in enumerate(zip(metrics, sims)):\n",
    "    #    if metric > threshold_metric:\n",
    "    #        filtered_ids.append(ix)\n",
    "    #    elif sim > threshold_sim:\n",
    "    #        filtered_ids.append(ix)\n",
    "                \n",
    "    print(f'{p:15s}total ids: {len(metrics)}\\t|\\t After filter: {len(filtered_ids)}')\n",
    "    return filtered_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tags(orig_sentences, trns_sentences): \n",
    "    tags_tr = []\n",
    "    for sent in (trns_sentences):\n",
    "        if len(sent):\n",
    "            tag_tr = []\n",
    "            temp = nlp_pt(str(sent).strip())\n",
    "            for token in temp:\n",
    "                tag_tr.append([token.text, token.pos_])\n",
    "            tags_tr.append(tag_tr)\n",
    "        else:\n",
    "            tags_tr.append(['invalid', 'invalid'])\n",
    "    \n",
    "    tags_or = []\n",
    "    for sent in (orig_sentences):\n",
    "        if len(sent):\n",
    "            tag_or = []\n",
    "            temp = nlp_en(str(sent).strip())\n",
    "            for token in temp:\n",
    "                tag_or.append([token.text, token.pos_])\n",
    "            tags_or.append(tag_or)\n",
    "        else:\n",
    "            tags_or.append(['invalid', 'invalid'])\n",
    "        \n",
    "    return tags_or, tags_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_index_by_porc(metrics, sims, porc, p=''):\n",
    "    \n",
    "    ordered_indexes = np.argsort(-metrics)\n",
    "    filtered_ids = ordered_indexes[:int(len(ordered_indexes) * porc)]\n",
    "                \n",
    "    print(f'{p:15s}total ids: {len(metrics)}\\t|\\t After filter: {len(filtered_ids)}')\n",
    "    return filtered_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_index_by_mean(metrics, sims, p=''):\n",
    "    \n",
    "    filtered_ids = []\n",
    "    if p == 'm':\n",
    "        threshold_metric = np.mean(metrics) - np.std(metrics)\n",
    "    else:\n",
    "        threshold_metric = np.mean(metrics)# + np.std(metrics)\n",
    "    threshold_sim = np.mean(sims)# + np.std(sims)\n",
    "    #ordered_indexes = np.argsort(-metrics)\n",
    "    #filtered_ids = ordered_indexes[:int(len(ordered_indexes) * porc)]\n",
    "    for ix, (metric, sim) in enumerate(zip(metrics, sims)):\n",
    "        if metric > threshold_metric:\n",
    "            filtered_ids.append(ix)\n",
    "        #elif sim > threshold_sim:\n",
    "        #    filtered_ids.append(ix)\n",
    "                \n",
    "    print(f'{p:15s}total ids: {len(metrics)}\\t|\\t After filter: {len(filtered_ids)}')\n",
    "    return filtered_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_senses = {}\n",
    "with open(Path.cwd() / 'data/wordnet/index.sense') as f:\n",
    "    for line in f.read().split('\\n'):\n",
    "        if len(line) > 1:\n",
    "            wordnet_senses[line.split()[0]] = line.split()[1]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_senses(path):\n",
    "    d_senses = {}\n",
    "    with open(Path.cwd() / path, 'r') as f:\n",
    "        for line in f.read().split('\\n'):\n",
    "            if len(line):\n",
    "                temp = line.split()\n",
    "                d_senses[temp[0]] = temp[1:]\n",
    "    return d_senses\n",
    "                \n",
    "d_senses_semcor = load_senses('data/WSD_Evaluation_Framework/Training_Corpora/SemCor/semcor.gold.key.txt')\n",
    "d_senses_omsti = load_senses('data/WSD_Evaluation_Framework/Training_Corpora/SemCor+OMSTI/semcor+omsti.gold.key.txt')\n",
    "d_senses_dev = load_senses('data/WSD_Evaluation_Framework/Evaluation_Datasets/ALL/all.gold.key.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_or_semcor = np.load('data/serialize/pos_or_semcor.npy')\n",
    "pos_tr_semcor = np.load('data/serialize/pos_tr_semcor.npy')\n",
    "\n",
    "pos_or_omsti = np.load('data/serialize/pos_or_omsti.npy')\n",
    "pos_tr_omsti = np.load('data/serialize/pos_tr_omsti.npy')\n",
    "\n",
    "pos_or_dev = np.load('data/serialize/pos_or_dev.npy')\n",
    "pos_tr_dev = np.load('data/serialize/pos_tr_dev.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pos_or_dev, pos_tr_dev = get_pos_tags(input_sent_dev, tr_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose=False\n",
    "tagged_tr_semcor = tagged_translations(list(range(0, len(id_semcor))), id_semcor, al_semcor, tr_semcor, input_sent_semcor, sent_to_id_semcor, target_sent_semcor, pos_or_semcor, pos_tr_semcor)\n",
    "tagged_tr_omsti = tagged_translations(list(range(0, len(id_omsti))), id_omsti, al_omsti, tr_omsti, input_sent_omsti, sent_to_id_omsti, target_sent_omsti, pos_or_omsti, pos_tr_omsti)\n",
    "tagged_tr_dev = tagged_translations(list(range(0, len(id_dev))), id_dev, al_dev, tr_dev, input_sent_dev, sent_to_id_dev, target_sent_dev, pos_or_dev, pos_tr_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_all = np.concatenate((tagged_tr_semcor, tagged_tr_omsti, tagged_tr_dev), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_disamb_all, ix_normal_all = count_instances_with_verbs(tagged_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-364-0f9ca625451b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mann_semcor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannotate_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_tr_semcor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_senses_semcor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msel_verbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mann_omsti\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannotate_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_tr_omsti\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_senses_omsti\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msel_verbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mann_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannotate_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_tr_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_senses_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msel_verbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-209-5d3c70e6825b>\u001b[0m in \u001b[0;36mannotate_sentences\u001b[0;34m(tagged_tr, d_senses, lemma, sel_verbs)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'no_instance'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordnet_senses\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0msel_verbs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0md_senses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwordnet_senses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnlp_pt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mPT_DICT_VERBS\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mwordnet_senses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md_senses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSELECTED_SYNSETS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                         \u001b[0mtemp_o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordnet_senses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md_senses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mlemma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable)\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__call__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.parse_batch\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.get_batch_model\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/api.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcontinue_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/api.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(seqs_in, drop)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseqs_in\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         X, bp_layer = layer.begin_update(layer.ops.flatten(seqs_in, pad=pad),\n\u001b[0;32m--> 280\u001b[0;31m                                          drop=drop)\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbp_layer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/api.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcontinue_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/neural/_classes/resnet.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbp_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mresidual_bwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/api.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcontinue_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/neural/_classes/layernorm.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mXhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackprop_rescale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_begin_update_scale_shift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfinish_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/neural/_classes/layernorm.py\u001b[0m in \u001b[0;36m_begin_update_scale_shift\u001b[0;34m(self, input__BI)\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0msgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgradient__BI\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minput__BI\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mG\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinish_update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/describe.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, type)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mem\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/neural/mem.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims, initial)\u001b[0m\n\u001b[1;32m   2770\u001b[0m     \"\"\"\n\u001b[1;32m   2771\u001b[0m     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out, keepdims=keepdims,\n\u001b[0;32m-> 2772\u001b[0;31m                           initial=initial)\n\u001b[0m\u001b[1;32m   2773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;31m# This branch is needed for reductions like any which don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ann_semcor = annotate_sentences(tagged_tr_semcor, d_senses_semcor, lemma=False, sel_verbs=True)\n",
    "ann_omsti = annotate_sentences(tagged_tr_omsti, d_senses_omsti, lemma=False, sel_verbs=True)\n",
    "ann_dev = annotate_sentences(tagged_tr_dev, d_senses_dev, lemma=False, sel_verbs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sense_all = np.concatenate((ann_semcor[0], ann_omsti[0], ann_dev[0]), axis=0)\n",
    "input_sense_all = np.concatenate((ann_semcor[1], ann_omsti[1], ann_dev[1]), axis=0)\n",
    "orig_sent_all = np.concatenate((ann_semcor[2], ann_omsti[2], ann_dev[2]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint\n",
    "#np.save('data/serialize/f1_all.npy', f1_all)\n",
    "\n",
    "#np.save('data/serialize/meteor_semcor.npy', meteor_semcor)\n",
    "#np.save('data/serialize/meteor_omsti.npy', meteor_omsti)\n",
    "#np.save('data/serialize/meteor_dev.npy', meteor_dev)\n",
    "\n",
    "#np.save('data/serialize/sim_semcor.npy', sim_semcor)\n",
    "#np.save('data/serialize/sim_omsti.npy', sim_omsti)\n",
    "#np.save('data/serialize/sim_dev.npy', sim_dev)\n",
    "\n",
    "#np.save('data/serialize/ann_semcor.npy', ann_semcor)\n",
    "#np.save('data/serialize/ann_omsti.npy', ann_omsti)\n",
    "#np.save('data/serialize/ann_dev.npy', ann_dev)\n",
    "\n",
    "#np.save('data/serialize/tagged_tr_semcor.npy', tagged_tr_semcor)\n",
    "#np.save('data/serialize/tagged_tr_omsti.npy', tagged_tr_omsti)\n",
    "#np.save('data/serialize/tagged_tr_dev.npy', tagged_tr_dev)\n",
    "\n",
    "#np.save('data/serialize/ix_disamb_all.npy', ix_disamb_all)\n",
    "#np.save('data/serialize/ix_normal_all.npy', ix_normal_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Q3 METEOR : 19392\n",
      "> Q3 F1 : 19392\n",
      "INTERSECCION METEOR Y F1 : 14178\n",
      "ORACIONES AMBIGUAS EN INTERSECCION 4107\n",
      "ORACIONES AMBIGUAS EN TODO EL CORPUS 18160\n"
     ]
    }
   ],
   "source": [
    "quartile_f1 = np.percentile(np.sort(f1_all), 75)\n",
    "quartile_meteor = np.percentile(np.sort(meteor_all), 75)\n",
    "\n",
    "temp_ix_f1 = np.where(f1_all > quartile_f1)[0]\n",
    "temp_ix_meteor = np.where(meteor_all > quartile_meteor)[0]\n",
    "\n",
    "temp = np.intersect1d(temp_ix_f1, temp_ix_meteor)\n",
    "temp_ix_inters = np.intersect1d(temp, ix_disamb_all)\n",
    "print('> Q3 METEOR :', len(temp_ix_meteor))\n",
    "print('> Q3 F1 :', len(temp_ix_f1))\n",
    "print('INTERSECCION METEOR Y F1 :', len(temp))\n",
    "print('ORACIONES AMBIGUAS EN INTERSECCION', len(temp_ix_inters))\n",
    "print('ORACIONES AMBIGUAS EN TODO EL CORPUS', len(ix_disamb_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Q3 METEOR : 16358\n",
      "> Q3 F1 : 15506\n",
      "INTERSECCION METEOR Y F1 : 13139\n",
      "ORACIONES AMBIGUAS EN INTERSECCION 3784\n",
      "ORACIONES AMBIGUAS EN TODO EL CORPUS 18160\n"
     ]
    }
   ],
   "source": [
    "quartile_f1 = np.percentile(np.sort(f1_all[ix_disamb_all]), 75)\n",
    "quartile_meteor = np.percentile(np.sort(meteor_all[ix_disamb_all]), 75)\n",
    "\n",
    "temp_ix_f1 = np.where(f1_all > quartile_f1)[0]\n",
    "temp_ix_meteor = np.where(meteor_all > quartile_meteor)[0]\n",
    "\n",
    "temp = np.intersect1d(temp_ix_f1, temp_ix_meteor)\n",
    "ix_dev = np.intersect1d(temp, ix_disamb_all)\n",
    "print('> Q3 METEOR :', len(temp_ix_meteor))\n",
    "print('> Q3 F1 :', len(temp_ix_f1))\n",
    "print('INTERSECCION METEOR Y F1 :', len(temp))\n",
    "print('ORACIONES AMBIGUAS EN INTERSECCION', len(temp_ix_inters))\n",
    "print('ORACIONES AMBIGUAS EN TODO EL CORPUS', len(ix_disamb_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_train = [i  for i in range(len(f1_all)) if i not in ix_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_path = Path.cwd() / 'data/disambiguation/dev'\n",
    "save_tagged_sentence(dev_path, 'dev', tagged_sense_all.take(ix_dev), \n",
    "                                     input_sense_all.take(ix_dev),\n",
    "                                     orig_sent_all.take(ix_dev),\n",
    "                                     mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_train = f1_all.take(ix_train)\n",
    "meteor_train = meteor_all.take(ix_train)\n",
    "sim_train = sim_all.take(ix_train)\n",
    "\n",
    "tagged_sense_train = tagged_sense_all.take(ix_train)\n",
    "input_sense_train = input_sense_all.take(ix_train)\n",
    "orig_sent_train = orig_sent_all.take(ix_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73784"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(f1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_random(len_normal, len_disamb, len_verb, ix_normal, ix_disamb, ix_verb, seed):\n",
    "    np.random.seed(seed)\n",
    "    temp_normal = np.random.choice(len(ix_normal), len_normal, replace=False)\n",
    "    np.random.seed(seed)\n",
    "    temp_disamb = np.random.choice(len(ix_disamb), len_disamb, replace=False)\n",
    "    np.random.seed(seed)\n",
    "    temp_verb = np.random.choice(len(ix_verb), len_verb, replace=False)\n",
    "    return np.concatenate((np.array(ix_normal).take(temp_normal), \n",
    "                           np.array(ix_verb).take(temp_verb)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7308, 15915)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ix_disamb_train), len(ix_verbs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 14376)"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ix_disamb_train), len(ix_verbs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sense_aux = []\n",
    "tagged_sense_aux = []\n",
    "for ix in range(len(input_sense_train)):\n",
    "    s_in = []\n",
    "    s_out = []\n",
    "    if '_tag' in input_sense_train[ix]:\n",
    "        for ix_word, (word_in, word_out) in enumerate(zip(input_sense_train[ix].split(), tagged_sense_train[ix].split())):\n",
    "            if '_tag' in word:\n",
    "                if word_in.replace('_tag', '') in PT_VERBS:\n",
    "                    s_in.append(word_in)\n",
    "                    s_out.append(word_out)\n",
    "                else:\n",
    "                    s_in.append(word_in.replace('_tag', ''))\n",
    "                    s_out.append(word_in.replace('_tag', ''))\n",
    "            else:\n",
    "                s_in.append(word_in)\n",
    "                s_out.append(word_out)\n",
    "        input_sense_aux.append(' '.join(s_in))\n",
    "        tagged_sense_aux.append(' '.join(s_out)) \n",
    "    else:\n",
    "        input_sense_aux.append(input_sense_train[ix])\n",
    "        tagged_sense_aux.append(tagged_sense_train[ix]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sense_aux = np.array(input_sense_aux)\n",
    "tagged_sense_aux = np.array(tagged_sense_aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_verbs_train = []\n",
    "ix_disamb_train = []\n",
    "for ix in range(len(tagged_sense_aux)):\n",
    "    if '_tag' in input_sense_aux[ix]:\n",
    "        flag = False\n",
    "        for word in input_sense_aux[ix].split():\n",
    "            if word.replace('_tag', '') in PT_VERBS:\n",
    "                flag = True\n",
    "                break\n",
    "        if flag:\n",
    "            ix_verbs_train.append(ix)\n",
    "        else:\n",
    "            ix_disamb_train.append(ix)\n",
    "        \n",
    "ix_normal_train = []\n",
    "for ix in range(len(tagged_sense_aux)):\n",
    "    if '_tag' not in input_sense_aux[ix]:\n",
    "        ix_normal_train.append(ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-369-ae3b0c48cfd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'f1' is not defined"
     ]
    }
   ],
   "source": [
    "len(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_temp(f1, meteor, sim, ix_normal, ix_disamb, ix_verbs, tagged_sense_aux, input_sense_aux, orig_sent_train):\n",
    "    for ter in [0.4]:\n",
    "        print(f'particion {100 * ter}. ')\n",
    "        filtered_ids_f = filter_index_by_porc(f1, sim, ter, p='f1')\n",
    "        filtered_ids_m = filter_index_by_porc(meteor, sim, ter, p='m')\n",
    "\n",
    "        # Numero de instancias ambiguas\n",
    "        n_amb_f = np.intersect1d(filtered_ids_f, ix_disamb)\n",
    "        n_ver_f = np.intersect1d(filtered_ids_f, ix_verbs)\n",
    "\n",
    "        n_amb_m = np.intersect1d(filtered_ids_m, ix_disamb)\n",
    "        n_ver_m = np.intersect1d(filtered_ids_m, ix_verbs)\n",
    "    return filtered_ids_f, filtered_ids_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "particion 40.0. \n",
      "f1             total ids: 73784\t|\t After filter: 29513\n",
      "m              total ids: 73784\t|\t After filter: 29513\n"
     ]
    }
   ],
   "source": [
    "filtered_ids_f, filtered_ids_m = gen_temp(f1_train, meteor_train, sim_train,\n",
    "                    ix_normal_train, ix_disamb_train, ix_verbs_train,\n",
    "                   tagged_sense_aux, input_sense_aux, orig_sent_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29513"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(orig_sent_train.take(filtered_ids_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29513"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(orig_sent_train.take(filtered_ids_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_f = (list(set(orig_sent_train.take(filtered_ids_f))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_m = (list(set(orig_sent_train.take(filtered_ids_m))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77568"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(or_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trns_f = []\n",
    "for i in range(len(ix_f)):\n",
    "    for ix in range(len(or_all)):\n",
    "        if or_all[i] == ix_f[i]:\n",
    "            trns_f.append(tr_all[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_f[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trns_f[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'meetings with donors that were launched during the final quarter of 2008 with the aim of providing a detailed explanation of the revised technical assistance strategy of the executive directorate and of discussing areas in which they might be able to provide technical assistance , will continue during 2009 .'"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how long has it been since you reviewed the objectives of your benefit and service program ?'"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_sent_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate_partitions(f1, meteor, sim, ix_normal, ix_disamb, ix_verbs, tagged_sense_aux, input_sense_aux, orig_sent_train):\n",
    "    for ter in [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "        print(f'particion {100 * ter}. ')\n",
    "        filtered_ids_f = filter_index_by_porc(f1, sim, ter, p='f1')\n",
    "        filtered_ids_m = filter_index_by_porc(meteor, sim, ter, p='m')\n",
    "\n",
    "        # Numero de instancias ambiguas\n",
    "        n_amb_f = np.intersect1d(filtered_ids_f, ix_disamb)\n",
    "        n_ver_f = np.intersect1d(filtered_ids_f, ix_verbs)\n",
    "\n",
    "        n_amb_m = np.intersect1d(filtered_ids_m, ix_disamb)\n",
    "        n_ver_m = np.intersect1d(filtered_ids_m, ix_verbs)\n",
    "        print(f'{len(n_ver_f)} oraciones ambiguas en f1, {len(n_ver_m)} oraciones ambiguas en meteor')\n",
    "\n",
    "        temp_path = Path.cwd() / f'data/disambiguation/all_{int(ter * 100)}'\n",
    "        save_tagged_sentence(temp_path, 'all_f1_lemma', tagged_sense_aux.take(filtered_ids_f), input_sense_aux.take(filtered_ids_f), orig_sent_train.take(filtered_ids_f))\n",
    "\n",
    "        ix_random = sampling_random(len(filtered_ids_f) - (len(n_amb_f) + len(n_ver_f)), \n",
    "                                    len(n_amb_f),\n",
    "                                    len(n_ver_f),\n",
    "                                    ix_normal, \n",
    "                                    ix_disamb, \n",
    "                                    ix_verbs,\n",
    "                                    SEED+30)\n",
    "        save_tagged_sentence(temp_path, 'all_f1_random_lemma', tagged_sense_aux.take(ix_random), input_sense_aux.take(ix_random), orig_sent_train.take(ix_random))\n",
    "        np.save(temp_path / 'r_b.npy', f1.take(filtered_ids_f))\n",
    "        np.save(temp_path / 'meteor.npy', meteor.take(filtered_ids_m))\n",
    "        save_tagged_sentence(temp_path, 'all_meteor_lemma', tagged_sense_aux.take(filtered_ids_m), input_sense_aux.take(filtered_ids_m), orig_sent_train.take(filtered_ids_m))\n",
    "\n",
    "        ix_random = sampling_random(len(filtered_ids_m) - (len(n_amb_m) + len(n_ver_m)), \n",
    "                                    len(n_amb_m),\n",
    "                                    len(n_ver_m),\n",
    "                                    ix_normal, \n",
    "                                    ix_disamb, \n",
    "                                    ix_verbs,\n",
    "                                    SEED+30)\n",
    "        save_tagged_sentence(temp_path, 'all_meteor_random_lemma', tagged_sense_aux.take(ix_random), input_sense_aux.take(ix_random), orig_sent_train.take(ix_random))\n",
    "\n",
    "        print()\n",
    "\n",
    "    print('all eq partition')\n",
    "\n",
    "    filtered_ids_f = filter_index_by_mean(f1, sim, p='f1')\n",
    "    filtered_ids_m = filter_index_by_mean(meteor, sim, p='m')\n",
    "\n",
    "    # Numero de instancias ambiguas\n",
    "    n_amb_f = np.intersect1d(filtered_ids_f, ix_disamb)\n",
    "    n_ver_f = np.intersect1d(filtered_ids_f, ix_verbs)\n",
    "\n",
    "    n_amb_m = np.intersect1d(filtered_ids_m, ix_disamb)\n",
    "    n_ver_m = np.intersect1d(filtered_ids_m, ix_verbs)\n",
    "    print(f'{len(n_ver_f)} oraciones ambiguas en f1, {len(n_ver_m)} oraciones ambiguas en meteor')\n",
    "\n",
    "    temp_path = Path.cwd() / f'data/disambiguation/all_eq'\n",
    "    print('Saving rouge and bleu')\n",
    "    save_tagged_sentence(temp_path, 'all_f1_lemma', tagged_sense_aux.take(filtered_ids_f), input_sense_aux.take(filtered_ids_f), orig_sent_train.take(filtered_ids_f))\n",
    "\n",
    "    ix_random = sampling_random(len(filtered_ids_f) - (len(n_amb_f) + len(n_ver_f)), \n",
    "                                len(n_amb_f),\n",
    "                                len(n_ver_f),\n",
    "                                ix_normal, \n",
    "                                ix_disamb, \n",
    "                                ix_verbs,\n",
    "                                SEED+30)\n",
    "    save_tagged_sentence(temp_path, 'all_f1_random_lemma', tagged_sense_aux.take(ix_random), input_sense_aux.take(ix_random), orig_sent_train.take(ix_random))\n",
    "\n",
    "    print('Saving meteor')\n",
    "    save_tagged_sentence(temp_path, 'all_meteor_lemma', tagged_sense_aux.take(filtered_ids_m), input_sense_aux.take(filtered_ids_m), orig_sent_train.take(filtered_ids_m))\n",
    "\n",
    "    ix_random = sampling_random(len(filtered_ids_m) - (len(n_amb_m) + len(n_ver_m)), \n",
    "                                len(n_amb_m),\n",
    "                                len(n_ver_m),\n",
    "                                ix_normal, \n",
    "                                ix_disamb, \n",
    "                                ix_verbs,\n",
    "                                SEED+30)\n",
    "    save_tagged_sentence(temp_path, 'all_meteor_random_lemma', tagged_sense_aux.take(ix_random), input_sense_aux.take(ix_random), orig_sent_train.take(ix_random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "particion 20.0. \n",
      "f1             total ids: 73784\t|\t After filter: 14756\n",
      "m              total ids: 73784\t|\t After filter: 14756\n",
      "1657 oraciones ambiguas en f1, 1250 oraciones ambiguas en meteor\n",
      "\n",
      "particion 30.0. \n",
      "f1             total ids: 73784\t|\t After filter: 22135\n",
      "m              total ids: 73784\t|\t After filter: 22135\n",
      "3801 oraciones ambiguas en f1, 3283 oraciones ambiguas en meteor\n",
      "\n",
      "particion 40.0. \n",
      "f1             total ids: 73784\t|\t After filter: 29513\n",
      "m              total ids: 73784\t|\t After filter: 29513\n",
      "5695 oraciones ambiguas en f1, 5143 oraciones ambiguas en meteor\n",
      "\n",
      "particion 50.0. \n",
      "f1             total ids: 73784\t|\t After filter: 36892\n",
      "m              total ids: 73784\t|\t After filter: 36892\n",
      "7394 oraciones ambiguas en f1, 6880 oraciones ambiguas en meteor\n",
      "\n",
      "particion 60.0. \n",
      "f1             total ids: 73784\t|\t After filter: 44270\n",
      "m              total ids: 73784\t|\t After filter: 44270\n",
      "8919 oraciones ambiguas en f1, 8540 oraciones ambiguas en meteor\n",
      "\n",
      "particion 70.0. \n",
      "f1             total ids: 73784\t|\t After filter: 51648\n",
      "m              total ids: 73784\t|\t After filter: 51648\n",
      "10440 oraciones ambiguas en f1, 10102 oraciones ambiguas en meteor\n",
      "\n",
      "all eq partition\n",
      "f1             total ids: 73784\t|\t After filter: 40870\n",
      "m              total ids: 73784\t|\t After filter: 68757\n",
      "8241 oraciones ambiguas en f1, 13488 oraciones ambiguas en meteor\n",
      "Saving rouge and bleu\n",
      "Saving meteor\n"
     ]
    }
   ],
   "source": [
    "def sampling_random(len_normal, len_disamb, len_verb, ix_normal, ix_disamb, ix_verb, seed):\n",
    "    np.random.seed(seed)\n",
    "    temp_normal = np.random.choice(len(ix_normal), len_normal, replace=False)\n",
    "    np.random.seed(seed)\n",
    "    temp_disamb = np.random.choice(len(ix_disamb), len_disamb, replace=False)\n",
    "    np.random.seed(seed)\n",
    "    temp_verb = np.random.choice(len(ix_verb), len_verb, replace=False)\n",
    "    return np.concatenate((np.array(ix_normal).take(temp_normal), \n",
    "                           np.array(ix_verb).take(temp_verb)), axis=0)\n",
    "\n",
    "generate_partitions(f1_train, meteor_train, sim_train,\n",
    "                    ix_normal_train, ix_disamb_train, ix_verbs_train,\n",
    "                   tagged_sense_aux, input_sense_aux, orig_sent_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'você tem permitir que ele se tornasse um programa de doação, ao invés de uma, que 02630189 a meta de melhoria do moral dos funcionários e , consequentemente , o aumento da produtividade ?'"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sense_aux.take(ix_verbs_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "particion 20.0. \n",
      "f1             total ids: 14376\t|\t After filter: 2875\n",
      "m              total ids: 14376\t|\t After filter: 2875\n",
      "2875 oraciones ambiguas en f1, 2875 oraciones ambiguas en meteor\n",
      "\n",
      "particion 30.0. \n",
      "f1             total ids: 14376\t|\t After filter: 4312\n",
      "m              total ids: 14376\t|\t After filter: 4312\n",
      "4312 oraciones ambiguas en f1, 4312 oraciones ambiguas en meteor\n",
      "\n",
      "particion 40.0. \n",
      "f1             total ids: 14376\t|\t After filter: 5750\n",
      "m              total ids: 14376\t|\t After filter: 5750\n",
      "5750 oraciones ambiguas en f1, 5750 oraciones ambiguas en meteor\n",
      "\n",
      "particion 50.0. \n",
      "f1             total ids: 14376\t|\t After filter: 7188\n",
      "m              total ids: 14376\t|\t After filter: 7188\n",
      "7188 oraciones ambiguas en f1, 7188 oraciones ambiguas en meteor\n",
      "\n",
      "particion 60.0. \n",
      "f1             total ids: 14376\t|\t After filter: 8625\n",
      "m              total ids: 14376\t|\t After filter: 8625\n",
      "8625 oraciones ambiguas en f1, 8625 oraciones ambiguas en meteor\n",
      "\n",
      "particion 70.0. \n",
      "f1             total ids: 14376\t|\t After filter: 10063\n",
      "m              total ids: 14376\t|\t After filter: 10063\n",
      "10063 oraciones ambiguas en f1, 10063 oraciones ambiguas en meteor\n",
      "\n",
      "all eq partition\n",
      "f1             total ids: 14376\t|\t After filter: 8334\n",
      "m              total ids: 14376\t|\t After filter: 13280\n",
      "8334 oraciones ambiguas en f1, 13280 oraciones ambiguas en meteor\n",
      "Saving rouge and bleu\n",
      "Saving meteor\n"
     ]
    }
   ],
   "source": [
    "def sampling_random(len_normal, len_disamb, len_verb, ix_normal, ix_disamb, ix_verb, seed):\n",
    "    np.random.seed(seed)\n",
    "    temp_normal = np.random.choice(len(ix_normal), len_normal, replace=False)\n",
    "    np.random.seed(seed)\n",
    "    temp_disamb = np.random.choice(len(ix_disamb), len_disamb, replace=False)\n",
    "    np.random.seed(seed)\n",
    "    temp_verb = np.random.choice(len(ix_verb), len_verb, replace=False)\n",
    "    return np.array(ix_verb).take(temp_verb)\n",
    "\n",
    "generate_partitions(f1_train.take(ix_verbs_train), meteor_train.take(ix_verbs_train), sim_train.take(ix_verbs_train),\n",
    "                    [], [], list(range(len(ix_verbs_train))),\n",
    "                   tagged_sense_aux.take(ix_verbs_train), input_sense_aux.take(ix_verbs_train), orig_sent_train.take(ix_verbs_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000.0"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "15000 * 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1             total ids: 73784\t|\t After filter: 40870\n",
      "m              total ids: 73784\t|\t After filter: 68757\n",
      "8241 oraciones ambiguas en f1, 13488 oraciones ambiguas en meteor\n",
      "Saving rouge and bleu\n",
      "Saving meteor\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50720"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_ids_m) - (len(n_amb_m) + len(n_ver_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50561"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ix_normal_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "prepare_data() got an unexpected keyword argument 'dir_dev'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-274-0cf1ebbf5e5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         _, _, pairs_train, pairs_dev, pairs_test, _= prepare_data(name_file, 'verbs_selected_lemma',\n\u001b[1;32m     11\u001b[0m                                                                        \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdir_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                                                                        dir_dev=dir_dev, dir_test=dir_test)\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_file\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.tsv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: prepare_data() got an unexpected keyword argument 'dir_dev'"
     ]
    }
   ],
   "source": [
    "from src.data import prepare_data\n",
    "\n",
    "dir_files = 'data/disambiguation/'\n",
    "dir_test = os.path.join(dir_files, 'test')\n",
    "dir_dev = os.path.join(dir_files, 'dev')\n",
    "\n",
    "for name_dir in ['all_20', 'all_30', 'all_40', 'all_50', 'all_60', 'all_eq', 'all_less']:\n",
    "    dir_train = os.path.join(dir_files, name_dir)\n",
    "    for name_file in ['all_f1_lemma', 'all_f1_random_lemma', 'all_meteor_lemma', 'all_meteor_random_lemma']:\n",
    "        _, _, pairs_train, pairs_dev, pairs_test, _= prepare_data(name_file, 'verbs_selected_lemma',\n",
    "                                                                       max_length=100, dir_train=dir_train, \n",
    "                                                                       dir_dev=dir_dev, dir_test=dir_test)\n",
    "        \n",
    "        pd.DataFrame(pairs_train).to_csv(os.path.join(dir_train, name_file + '.tsv'), header=False, index=False, sep='\\t')\n",
    "\n",
    "pd.DataFrame(pairs_dev).to_csv(os.path.join(dir_dev, 'verbs_selected_lemma.tsv'), header=False, index=False, sep='\\t')\n",
    "pd.DataFrame(pairs_test).to_csv(os.path.join(dir_test, 'verbs_selected_lemma.tsv'), header=False, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
