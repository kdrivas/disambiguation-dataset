{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import fire\n",
    "import random\n",
    "import os\n",
    "\n",
    "from model.encoder import Encoder\n",
    "from model.decoder import Decoder_luong\n",
    "from model.seq2seq import Seq2seq\n",
    "\n",
    "from src.utils import time_since\n",
    "from src.utils import get_stats\n",
    "from src.data import prepare_data\n",
    "from src.data_loader import get_loader\n",
    "from src.evaluator import evaluate_acc\n",
    "\n",
    "def train(input_var, target_var, model, model_optimzier, clip, train=True):\n",
    "    \n",
    "    if train:\n",
    "        model_optimzier.zero_grad()\n",
    "    \n",
    "    all_decoder_outputs, target_var = model(input_var, target_var, train)\n",
    "    loss = nn.NLLLoss()(all_decoder_outputs.view(-1, decoder.output_size), target_var.contiguous().view(-1))          \n",
    "    \n",
    "    if train:\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        model_optimzier.step()\n",
    "    \n",
    "    return loss.item() \n",
    "\n",
    "def main(dir_files='data/disambiguation/', dir_results='results/', max_length=120, cuda_ids = [0, 1], cuda=True, seed=0):\n",
    "    \n",
    "    dir_train = os.path.join(dir_files, 'all')\n",
    "    dir_test = os.path.join(dir_files, 'test')\n",
    "    dir_results = os.path.join(dir_files, name_file)\n",
    "    os.mkdir(dir_results)\n",
    "    \n",
    "    attn_model = 'general'\n",
    "    hidden_size = 512\n",
    "    emb_size = 400\n",
    "    n_layers = 2\n",
    "    seed = 12\n",
    "    dropout_p = 0.2\n",
    "    tf_ratio = 0.5\n",
    "    clip = 5.0\n",
    "\n",
    "    n_epochs = 10\n",
    "    batch_size = 50\n",
    "    plot_every = 5\n",
    "    start_eval = 5\n",
    "    print_every = 5\n",
    "    validate_loss_every = 100\n",
    "    evaluate_every = 25\n",
    "\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "    validation_acc = []\n",
    "    best_metric = 0\n",
    "    print_loss_total = 0\n",
    "    plot_loss_total = 0 \n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "    input_lang, output_lang, pairs_train, pairs_test, senses_per_sentence = prepare_data('all_f1_lemma', 'verbs_selected_lemma', max_length=max_length, dir_train=dir_train, dir_test=dir_test)\n",
    "    selected_synsets = np.load(dir_files / 'selected_synsets.npy')\n",
    "    \n",
    "    encoder = Encoder(len(input_lang.vocab.stoi), hidden_size, emb_size, n_layers, dropout_p, USE_CUDA=cuda)\n",
    "    decoder = Decoder_luong(attn_model, hidden_size, len(output_lang.vocab.stoi), emb_size, 2 * n_layers, dropout_p, USE_CUDA=cuda)\n",
    "    model = Seq2seq(input_lang, output_lang, encoder, decoder, tf_ratio, cuda)\n",
    "\n",
    "    if cuda:\n",
    "        model = nn.DataParallel(model, device_ids=[cuda_ids]).cuda()\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    model_optimizer = optim.Adam(model.parameters())\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    train_loader = get_loader(pairs_train, input_lang.vocab.stoi, output_lang.vocab.stoi, batch_size=batch_size)\n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(1, n_epochs): \n",
    "        # Shuffle data\n",
    "        id_aux = np.random.permutation(np.arange(len(pairs_train)))\n",
    "        pairs_train = pairs_train[id_aux]\n",
    "\n",
    "        model.train()\n",
    "        print_loss_total = 0\n",
    "        # Get the batches for this epoch\n",
    "\n",
    "        for batch_ix, (input_var, _, target_var, _) in enumerate(train_loader):\n",
    "            # Transfer to GPU\n",
    "            input_var, target_var = input_var.to(device), target_var.to(device)\n",
    "\n",
    "            # Run the train function\n",
    "            loss = train(input_var, target_var, model, model_optimizer, clip, train=train)            \n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Keep track of loss\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "\n",
    "            if batch_ix == 0 and epoch == 1: continue\n",
    "\n",
    "            if batch_ix % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print_summary = '%s (%d %d%%) %.4f' % (time_since(start, epoch / n_epochs), epoch, 100 * batch_ix / round(len(pairs_train) / batch_size), print_loss_avg)\n",
    "                train_losses.append(loss)\n",
    "                print(print_summary)\n",
    "\n",
    "            if epoch >= 2 and batch_ix % evaluate_every == 0:\n",
    "                model.eval()\n",
    "\n",
    "                metric = evaluate_acc(pairs_test, senses_per_sentence, k_beams=1, verbose=False)\n",
    "                if metric > best_metric:\n",
    "                    best_metric = metric\n",
    "                    torch.save(model.state_dict(), f'{dir_results}/seq2seq.pkl')\n",
    "                    torch.save(encoder.state_dict(), f'{dir_results}/enc.pkl')\n",
    "                    torch.save(decoder.state_dict(), f'{dir_results}/dec.pkl')\n",
    "                    print('Saving weights')\n",
    "                validation_acc.append(acc)\n",
    "                print(f'------------- metric: {acc}')\n",
    "\n",
    "                model.train()\n",
    "                \n",
    "                \n",
    "    np.save(f'{dir_results}/train_losses.npy', train_losses)\n",
    "    np.save(f'{dir_results}/validation_losses.npy', validation_losses)\n",
    "    np.save(f'{dir_results}/validation_acc', validation_acc)\n",
    "\n",
    "    model.load_state_dict(torch.load(f'{dir_results}/seq2seq.pkl'))\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    f1, precision, recall, report = evaluate_acc(encoder, decoder, input_lang, output_lang, pairs_test, selected_synsets, senses_per_sentence, k_beams=1, report=True, max_length=max_length, cuda=cuda)\n",
    "    print('f1 score:', f1, 'precision:', precision, 'recall:', recall)\n",
    "\n",
    "    res = get_stats(report, pairs_train, pairs_test)\n",
    "    res.to_csv(f'{dir_results}/report.csv')\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    fire.Fire(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Indexed 57064 words in input language, 56367 words in output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
